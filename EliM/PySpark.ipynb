{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PySpark Research**\n",
    "\n",
    "Apache Spark is an engine used to process data at a large scale efficiently. It has APIs in Python and R. PySpark is the Python API for Apache Spark. PySpark has features which include Spark SQL, dataframes and machine learning.\n",
    "\n",
    "Using PySpark dataframes allows us to efficienetly analyse and tranform data iby using Python and Spark SQL together. Spark SQL is the Apache Spark module for using structured data, like dataframes.\n",
    "\n",
    "There is a Pandas API for Spark which may be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Tests\n",
    "After installing java and pyspark and setting the environment variables correctly, I will test out pyspark to see if it works in my notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840668 sha256=f80666a739730b9ec777223adcf1ec93d269bf0eadcbe5e4c43dbb82061360b2\n",
      "  Stored in directory: c:\\users\\milse\\appdata\\local\\pip\\cache\\wheels\\07\\a0\\a3\\d24c94bf043ab5c7e38c30491199a2a11fef8d2584e6df7fb7\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use SparkSession to initalise the entry point of PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a Spark session\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"TestApp\").getOrCreate()\n",
    "\n",
    "# Test by printing the Spark version\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.4\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark works for this version of python, but for more recent version 3.12 onwards it did not seem to work. Hence, we will use python 3.11.4 in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use spark.createDataFrame to create a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|Cathy| 29|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a simple DataFrame\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use .show() or display() to visualise data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Perform a simple operation\n",
    "df.filter(df.Age > 30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the data is given using describe() and show()\n",
    "We can use collect() to show the dataframe in terms of rows and column variables.\n",
    "\n",
    "To convert spark dataframes to pandas, use toPandas().\n",
    "\n",
    "select() chooses column from the dataframes.\n",
    "\n",
    "withColumn() can add new columns.\n",
    "\n",
    "groupby() to group data\n",
    "\n",
    "avg() to average data\n",
    "\n",
    "write.csv and read.csv to write and read in data.\n",
    "\n",
    "Functions can be imported from pyspark.sql.functions, commonly under alias 'F'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have investigated the basics of Apache Spark and the PySpark API. We will refer back to the documentation for use of any functions we will need in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "[1] PySpark Documentation: https://spark.apache.org/docs/latest/api/python/user_guide/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
