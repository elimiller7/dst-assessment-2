{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv('../unpushed_work/last_fm_data/user_artists.dat', sep='\\t')\n",
    "\n",
    "user_interaction_counts = user_data.groupby('userID').size()\n",
    "users_with_50_interactions = user_interaction_counts[user_interaction_counts >= 50].index\n",
    "user_data_filtered = user_data[user_data['userID'].isin(users_with_50_interactions)]\n",
    "\n",
    "unique_users = user_data_filtered['userID'].unique()\n",
    "\n",
    "# Ensure that test users have at least 50 interactions in 'test_data'\n",
    "# We need to carefully select 'test_users' to satisfy this condition\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "shuffled_users = np.random.permutation(unique_users)\n",
    "\n",
    "train_users = []\n",
    "test_users = []\n",
    "\n",
    "# We'll collect test users until we have enough that have at least 50 interactions\n",
    "for user in shuffled_users:\n",
    "    user_data_temp = user_data_filtered[user_data_filtered['userID'] == user]\n",
    "    if len(test_users) < int(0.2 * len(unique_users)):\n",
    "        # Tentatively add to test_users\n",
    "        test_users.append(user)\n",
    "    else:\n",
    "        train_users.append(user)\n",
    "\n",
    "# Recreate test_data and train_data\n",
    "train_data = user_data_filtered[user_data_filtered['userID'].isin(train_users)]\n",
    "test_data = user_data_filtered[user_data_filtered['userID'].isin(test_users)]\n",
    "\n",
    "# Now check that each user in test_data has 50 interactions\n",
    "# Remove any users from test_users who don't meet this criterion\n",
    "valid_test_users = []\n",
    "for user in test_users:\n",
    "    user_data_temp = test_data[test_data['userID'] == user]\n",
    "    if len(user_data_temp) == 50:\n",
    "        valid_test_users.append(user)\n",
    "\n",
    "# Update test_users and test_data\n",
    "test_users = valid_test_users\n",
    "test_data = test_data[test_data['userID'].isin(test_users)]\n",
    "\n",
    "# Update train_data to include any users removed from test_users\n",
    "removed_test_users = set(shuffled_users) - set(train_users) - set(test_users)\n",
    "if removed_test_users:\n",
    "    train_users.extend(list(removed_test_users))\n",
    "    train_data = user_data_filtered[user_data_filtered['userID'].isin(train_users)]\n",
    "\n",
    "# Now proceed to split test_data into test_x and test_y\n",
    "test_x = pd.DataFrame(columns=test_data.columns)\n",
    "test_y = pd.DataFrame(columns=test_data.columns)\n",
    "\n",
    "for user in test_users:\n",
    "    user_data_temp = test_data[test_data['userID'] == user]\n",
    "    user_data_shuffled = user_data_temp.sample(frac=1, random_state=42)\n",
    "    user_test_x = user_data_shuffled.iloc[:25]\n",
    "    user_test_y = user_data_shuffled.iloc[25:50]\n",
    "    test_x = pd.concat([test_x, user_test_x], ignore_index=True)\n",
    "    test_y = pd.concat([test_y, user_test_y], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode artistIDs\n",
    "# Essential since ML models require numerical input (also efficient)\n",
    "artist_encoder = LabelEncoder()\n",
    "all_artistIDs = user_data_filtered['artistID'].unique()\n",
    "artist_encoder.fit(all_artistIDs)\n",
    "num_artists = len(artist_encoder.classes_)\n",
    "\n",
    "# Encode train users\n",
    "train_user_encoder = LabelEncoder()\n",
    "train_user_encoder.fit(train_users)\n",
    "num_train_users = len(train_user_encoder.classes_)\n",
    "\n",
    "# Encode test users\n",
    "test_user_encoder = LabelEncoder()\n",
    "test_user_encoder.fit(test_users)\n",
    "num_test_users = len(test_user_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create user-item interaction matrix\n",
    "def create_user_item_matrix(data, user_encoder, num_users):\n",
    "    user_item_matrix = np.zeros((num_users, num_artists))\n",
    "    for idx, row in data.iterrows():\n",
    "        user_idx = user_encoder.transform([row['userID']])[0]\n",
    "        artist_idx = artist_encoder.transform([row['artistID']])[0]\n",
    "        weight = row['weight']\n",
    "        user_item_matrix[user_idx, artist_idx] = weight\n",
    "    return user_item_matrix\n",
    "\n",
    "train_user_item_matrix = create_user_item_matrix(train_data, train_user_encoder, num_train_users)\n",
    "test_x_user_item_matrix = create_user_item_matrix(test_x, test_user_encoder, num_test_users)\n",
    "test_y_user_item_matrix = create_user_item_matrix(test_y, test_user_encoder, num_test_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterise(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std) # Adds noise by sampling from standard normal dist.\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.fc_decode(z))\n",
    "        return torch.sigmoid(self.fc_out(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Convert train data to tensor\n",
    "train_tensor = torch.FloatTensor(train_user_item_matrix)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "train_dataset = torch.utils.data.TensorDataset(train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "input_dim = num_artists\n",
    "hidden_dim = 256\n",
    "latent_dim = 50\n",
    "\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data_batch in train_loader:\n",
    "        data = data_batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {train_loss / len(train_loader.dataset):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
