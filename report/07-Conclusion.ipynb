{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative Filtering:\n",
    "\n",
    "We investigated collaborative filtering through memory-based and model-based methods, implementing simple user-based and item-based methods which were slow, and model-based methods - SVD and ALS - which were much more efficient, and allowed quick recommendations to be given. All of these methods gave recommendations which ranged in quality and performance depending on the availability of data. Next, we used PySpark to implement ALS and this increased the computational efficiency significantly, allowing us to quickly give recommendations for all of the users. This feature would allow us to scale our recommendation methods to large datasets and would be vital in developing our music recommendation system, which could theoretically have millions of users. We evaluated the performance of the PySpark ALS method use RMSE and found this to be very high, leading us to conclude that there is significant room for improvement in our recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational autoencoders:\n",
    "\n",
    "We explored the use of Variational Autoencoders as a means of creating a recommender system for music artists. To do this, we refined our model by adjusting its hyperparameters and analysing its computational performance. Overall, we have seen that the model's hit rate and NDCG score are relatively low, but can be affected by optimising the use of batches, the activation function, the optimiser, and the number of layers in the VAE. Batch optimisation consisted of batch normalisation at each layer, and changing the batch size, and we also visualised the latent space for models with a varying number of layers to see how the depth of the VAE affected its ability to capture complex patterns in the latent space. Overall, the VAE performed worse than traditional models for Recommender Systems, potentially due to the hyperparameters still not being optimal, though due to time and memory constraints it was difficult to optimise these further. However, the MSE of the VAE model was lower than that of the purely random generator, suggesting that our model was able to successfully capture some relationships in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Neural Network:\n",
    "\n",
    "In section 5 we explored the use of Graph Neural Networks, particularly the LightGCN model, which was able to perform at a relatively high level, with recall around 0.27, and further predictions that were all similar, suggesting that the model successfully made links between similar artists. We also explored the use of GPU's in the model and how they would speed up the running of the model. Finally we looked at the resource usage of the model, mainly time requirements and GPU memory requirements."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
