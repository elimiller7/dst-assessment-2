{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Collaborative Filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 0: Theory\n",
    "\n",
    "Collaborative filtering creates a user-item matrix with values corresponding to the users preferences. Next, using a chosen similarity metric, the similarities between users' preferences are used to give recommendations for each user. Each user will be given recommendations for items that they have not given feedback for, but have positive feedback from users similar to the chosen user. These recommendations may also be predictions.\n",
    "\n",
    "### Similarity\n",
    "There are different similarity measures that can be chosen. The Pearson correlation coefficient measures linear relation between two variables, and the cosine similarity measures the simialrirty between two vectors depending on the angle between them in a vector space. Similarity may also be referenced as the distance metric or correlation metric.\n",
    "\n",
    "The two types of collaborative filtering techniques are model-based and memory-based.\n",
    "\n",
    "### Memory-based Methods\n",
    "Memory-based collaborative filtering can be user-based or item-based. User-based techniques compute the similarities between users based on their implicit feedback for the same item. Then, the predicted rating or given feedback is calculated using weighted averages of the item's ratings given by similar users. The weights are the similarities of the other users with the chosen item. Item-based techniques work similalrly but use the similarity between items instead of the similarity between users. Both of these methods form a similarity matrix.\n",
    "\n",
    "### Model-based Methods\n",
    "Model-based collaborative filtering can be a lot quicker than memory-based methods. An example of this is the singular value decomposition (SVD). These methods use the user-item matrix to find rules between items and uses these rules to give a list of recommendations. If data is sparse, then model-based methods are recommended to deal with this. More advanced model-based recommendation systems can use clustering, neural networks and elements of graph theory. The main drawback of model-based methods is that they are typically have a very high computational cost and may require a large amount of memory.\n",
    "\n",
    "The most popular algorithm used for collaborative filtering, when the similarity matrix is sparse, is Alternating Least Squares (ALS) minimisation. Simply, this aims to estimate the entries of a matrix $M=UV^T$ when only a subset of these entries is observed. The algorithm minimises the squared error with the observed entries, when alternating in optimising $U$ and $V$. This would allow us to give predicted entries for items which a given user has not listened to yet.\n",
    "\n",
    "### Pros & Cons\n",
    "Collaborative filtering can be used when data is difficult to analyse since it can use the imnplicit feedback. However, there are a few problems. Firstly, the cold-start problem - a new user has no data, hence, the system cannot make meaningful recommendations for them. Also, if data is sparse, then recommendations can be less accurate and many items may not be recommended at all. Finally, the method must be scalable in order to stay efficient. The basic collaborative filtering methods can struggle with this, but model-based methods like SVD can be used to give efficient and robust recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Data Preparation\n",
    "## 1.1 Loading Data & Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "import implicit\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "artists = pd.read_csv(os.path.join('..','data','artists.dat'), delimiter='\\t')\n",
    "tags = pd.read_csv(os.path.join('..','data','tags.dat'), delimiter='\\t',encoding='ISO-8859-1')\n",
    "user_artists = pd.read_csv(os.path.join('..','data','user_artists.dat'), delimiter='\\t')\n",
    "user_friends = pd.read_csv(os.path.join('..','data','user_friends.dat'), delimiter='\\t')\n",
    "user_taggedartists_timestamps = pd.read_csv(os.path.join('..','data','user_taggedartists-timestamps.dat'), delimiter='\\t')\n",
    "user_taggedartists = pd.read_csv(os.path.join('..','data','user_taggedartists.dat'), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns from the Artists dataset\n",
    "artists_cleaned = artists.drop(columns=['url', 'pictureURL']).drop_duplicates(keep='first') \n",
    "\n",
    "# Drop the irrelevant columns in the Tags dataset\n",
    "tags_cleaned = tags.drop_duplicates(keep='first') \n",
    "\n",
    "# For the User-Artists dataset, we can filter out rows with a weight of 0, as they show no meaningful interaction\n",
    "# user_artists_cleaned = user_artists[user_artists['weight'] > 0]\n",
    "user_artists_cleaned = user_artists.drop_duplicates(keep='first') \n",
    "\n",
    "# Drop duplicates from the User-Tagged Artists Timestamps dataset\n",
    "user_taggedartists_timestamps_cleaned = user_taggedartists_timestamps.drop_duplicates(keep='first') \n",
    "\n",
    "# Convert timestamps from ms to datetime format\n",
    "user_taggedartists_timestamps_cleaned['timestamp'] = pd.to_datetime(user_taggedartists_timestamps_cleaned['timestamp'], unit='ms')\n",
    "\n",
    "# Drop duplicates from the User-Friends dataset\n",
    "user_friends_cleaned = user_friends.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to output cleaned datasets for inspection\n",
    "# print(\"Cleaned Artists dataset:\", artists_cleaned.info(), artists_cleaned.head())\n",
    "# print(\"Cleaned Tags dataset:\", tags_cleaned.info(), tags_cleaned.head())\n",
    "# print(\"Cleaned User-Artists dataset:\", user_artists_cleaned.info(), user_artists_cleaned.head())\n",
    "# print(\"Cleaned User-Tagged Artists Timestamps dataset:\", user_taggedartists_timestamps_cleaned.info(), user_taggedartists_timestamps_cleaned.head())\n",
    "# print(\"Cleaned User-Friends dataset:\", user_friends_cleaned.info(), user_friends_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Implementing Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement collaborative filtering using some of the different techniques that we have described. We will use some memory-based and some model-based methods.\n",
    "\n",
    "First, we create a dictionary which will allow us to map artistID recommendations to the corresponding names of the artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map artistID to artistName\n",
    "artist_id_to_name = dict(zip(artists['id'], artists['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise the user-artist matrix which has values corresponding to the listening counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artistID  1      2      3      4      5      6      7      8      9      \\\n",
      "userID                                                                    \n",
      "2           0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "3           0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4           0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "5           0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "6           0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "...         ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "2095        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2096        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2097        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2099        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2100        0.0    0.0  408.0    0.0    0.0  404.0    0.0    0.0    0.0   \n",
      "\n",
      "artistID  10     ...  18736  18737  18738  18739  18740  18741  18742  18743  \\\n",
      "userID           ...                                                           \n",
      "2           0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "3           0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4           0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "5           0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "6           0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "...         ...  ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "2095        0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2096        0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2097        0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2099        0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2100        0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "artistID  18744  18745  \n",
      "userID                  \n",
      "2           0.0    0.0  \n",
      "3           0.0    0.0  \n",
      "4           0.0    0.0  \n",
      "5           0.0    0.0  \n",
      "6           0.0    0.0  \n",
      "...         ...    ...  \n",
      "2095        0.0    0.0  \n",
      "2096        0.0    0.0  \n",
      "2097        0.0    0.0  \n",
      "2099        0.0    0.0  \n",
      "2100        0.0    0.0  \n",
      "\n",
      "[1892 rows x 17632 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a user-artist interaction matrix using the user_artists_cleaned dataset\n",
    "user_artist_matrix = user_artists_cleaned.pivot(index='userID', columns='artistID', values='weight')\n",
    "\n",
    "# Fill NaN values with 0s (assuming binary or implicit feedback, i.e., 1 for interaction, 0 for no interaction)\n",
    "user_artist_matrix = user_artist_matrix.fillna(0)\n",
    "\n",
    "print(user_artist_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Memory-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 User-Based Implementation\n",
    "For the user-based implementation, we must compute the similarity matrix using the cosine similarity between users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userID      2     3         4         5         6         7         8     \\\n",
      "userID                                                                     \n",
      "2       1.000000   0.0  0.144786  0.028692  0.007016  0.030219  0.008964   \n",
      "3       0.000000   1.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4       0.144786   0.0  1.000000  0.081193  0.006609  0.000000  0.000000   \n",
      "5       0.028692   0.0  0.081193  1.000000  0.000000  0.000000  0.000000   \n",
      "6       0.007016   0.0  0.006609  0.000000  1.000000  0.012713  0.018881   \n",
      "\n",
      "userID  9         10        11    ...      2090      2091      2092      2093  \\\n",
      "userID                            ...                                           \n",
      "2        0.0  0.000000  0.021267  ...  0.000000  0.043405  0.000000  0.004625   \n",
      "3        0.0  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "4        0.0  0.009072  0.013407  ...  0.000000  0.000000  0.003776  0.006178   \n",
      "5        0.0  0.169078  0.004639  ...  0.010993  0.000000  0.205141  0.000000   \n",
      "6        0.0  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "userID      2094  2095      2096      2097      2099  2100  \n",
      "userID                                                      \n",
      "2       0.001585   0.0  0.000956  0.082134  0.000000   0.0  \n",
      "3       0.000000   0.0  0.000000  0.000000  0.000318   0.0  \n",
      "4       0.000000   0.0  0.045125  0.659085  0.000000   0.0  \n",
      "5       0.000000   0.0  0.204557  0.119133  0.000000   0.0  \n",
      "6       0.000000   0.0  0.000000  0.000000  0.000000   0.0  \n",
      "\n",
      "[5 rows x 1892 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compute the cosine similarity between users\n",
    "user_similarity = cosine_similarity(user_artist_matrix)\n",
    "\n",
    "# Convert the similarity matrix into a DataFrame for easy inspection\n",
    "user_similarity_df = pd.DataFrame(user_similarity, index=user_artist_matrix.index, columns=user_artist_matrix.index)\n",
    "\n",
    "# Display a portion of the user similarity matrix\n",
    "print(user_similarity_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function which gives user-based recommendations. This function uses the user-artist matrix and the similarity matrix to give the top-$N$ recommendations for the given user. It then maps the corresponding ID recommendations of artists to the artist names using the previously defined dictionary.\n",
    "\n",
    "The implementation is shown for user 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get user-based recommendations\n",
    "def get_user_based_recommendations(user_id, user_similarity_df, user_artist_matrix, artist_id_to_name, top_n=10):    \n",
    "    # Get the most similar users (excluding the user itself)\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False).index[1:]\n",
    "\n",
    "    recommendations = {}\n",
    "    for similar_user in similar_users:\n",
    "        # Get the artists this similar user has interacted with (non-zero values)\n",
    "        interacted_artists = user_artist_matrix.loc[similar_user][user_artist_matrix.loc[similar_user] > 0].index.tolist()\n",
    "\n",
    "        for artist in interacted_artists:\n",
    "            # Only consider artists the target user has not interacted with\n",
    "            if artist not in user_artist_matrix.loc[user_id][user_artist_matrix.loc[user_id] > 0].index.tolist():\n",
    "                # Add the artist to recommendations with a score (using the scaled similarity as a weight)\n",
    "                if artist not in recommendations:\n",
    "                    recommendations[artist] = user_similarity_df[user_id][similar_user]\n",
    "                else:\n",
    "                    # Add the weight of similarity to the current score\n",
    "                    recommendations[artist] += user_similarity_df[user_id][similar_user]\n",
    "\n",
    "    # Sort recommendations by score (highest first)\n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Convert artist IDs to names and prepare the final list with IDs, names, and scores\n",
    "    recommended_artists = [(artist, artist_id_to_name.get(artist, \"Unknown\"), score) for artist, score in sorted_recommendations[:top_n]]\n",
    "\n",
    "    return recommended_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top User-Based Recommendations for User 2:\n",
      "Artist ID: 289, Artist: Britney Spears, Similarity Score: 20.67\n",
      "Artist ID: 288, Artist: Rihanna, Similarity Score: 20.10\n",
      "Artist ID: 295, Artist: Beyoncé, Similarity Score: 16.92\n",
      "Artist ID: 292, Artist: Christina Aguilera, Similarity Score: 16.73\n",
      "Artist ID: 300, Artist: Katy Perry, Similarity Score: 15.50\n",
      "Time elapsed: 15.2663 seconds\n"
     ]
    }
   ],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 user-based recommendations for user with userID=2\n",
    "user_id = 2\n",
    "user_based_recommendations = get_user_based_recommendations(user_id, user_similarity_df, user_artist_matrix, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display user-based recommendations\n",
    "print(\"Top User-Based Recommendations for User 2:\")\n",
    "for artist_id, artist_name, score in user_based_recommendations:\n",
    "    print(f\"Artist ID: {artist_id}, Artist: {artist_name}, Similarity Score: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# calculate and display elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time elapsed: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the output is the top 5 recommendations for user 2 with their similarity scores. The output has recommended 5 female pop singers which is interesting. We will now give recommendations for user 3. We also take note of the time taken to give the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top User-Based Recommendations for User 3:\n",
      "Artist ID: 757, Artist: Crystal Castles, Similarity Score: 0.25\n",
      "Artist ID: 603, Artist: Aphex Twin, Similarity Score: 0.25\n",
      "Artist ID: 1222, Artist: Venetian Snares, Similarity Score: 0.23\n",
      "Artist ID: 2174, Artist: edIT, Similarity Score: 0.23\n",
      "Artist ID: 154, Artist: Radiohead, Similarity Score: 0.21\n",
      "Time elapsed: 15.0849 seconds\n"
     ]
    }
   ],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 user-based recommendations for user with userID=3\n",
    "user_id = 3\n",
    "user_based_recommendations = get_user_based_recommendations(user_id, user_similarity_df, user_artist_matrix, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display user-based recommendations\n",
    "print(\"Top User-Based Recommendations for User 3:\")\n",
    "for artist_id, artist_name, score in user_based_recommendations:\n",
    "    print(f\"Artist ID: {artist_id}, Artist: {artist_name}, Similarity Score: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# calculate and display elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time elapsed: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artists recommended for user 3 have much lower similarity scores than those recommended to user 2. In this case, the top 4 recommendations all could be categorised as electronic artists. However, the fifth recommendation 'Radiohead' is not very similar to the others.  We also take note of the time taken to give the recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Item-based Implementation\n",
    "For the item-based implementation, we must compute the similarity matrix using the cosine similarity between artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artistID  1        2      3      4        5         6         7         8      \\\n",
      "artistID                                                                        \n",
      "1           1.0  0.00000    0.0    0.0  0.00000  0.000000  0.008784  0.032075   \n",
      "2           0.0  1.00000    0.0    0.0  0.20774  0.000000  0.010696  0.000000   \n",
      "3           0.0  0.00000    1.0    0.0  0.00000  0.205607  0.000000  0.000000   \n",
      "4           0.0  0.00000    0.0    1.0  0.00000  0.000000  0.019742  0.049547   \n",
      "5           0.0  0.20774    0.0    0.0  1.00000  0.000000  0.042728  0.000000   \n",
      "\n",
      "artistID     9         10     ...  18736  18737  18738  18739  18740  18741  \\\n",
      "artistID                      ...                                             \n",
      "1         0.000000  0.000000  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2         0.102094  0.387653  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "3         0.000000  0.000000  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4         0.000000  0.000000  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "5         0.190713  0.000000  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "artistID  18742  18743  18744  18745  \n",
      "artistID                              \n",
      "1           0.0    0.0    0.0    0.0  \n",
      "2           0.0    0.0    0.0    0.0  \n",
      "3           0.0    0.0    0.0    0.0  \n",
      "4           0.0    0.0    0.0    0.0  \n",
      "5           0.0    0.0    0.0    0.0  \n",
      "\n",
      "[5 rows x 17632 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compute the cosine similarity between artists (transpose the matrix to compare artists)\n",
    "artist_similarity = cosine_similarity(user_artist_matrix.T)  # Transpose to compare artists (columns)\n",
    "\n",
    "# Convert the similarity matrix into a DataFrame for easy inspection\n",
    "artist_similarity_df = pd.DataFrame(artist_similarity, index=user_artist_matrix.columns, columns=user_artist_matrix.columns)\n",
    "\n",
    "# Display a portion of the artist similarity matrix\n",
    "print(artist_similarity_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function which gives item-based recommendations. This function uses the user-artist matrix and the similarity matrix to give the top-$N$ recommendations for the given user. It then maps the corresponding ID recommendations of artists to the artist names using the previously defined dictionary.\n",
    "\n",
    "The implementation is shown for user 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get item-based recommendations\n",
    "def get_item_based_recommendations(user_id, user_artist_matrix, artist_similarity_df, artist_id_to_name, top_n=10):\n",
    "    # Get the artists the user has interacted with (non-zero values)\n",
    "    interacted_artists = user_artist_matrix.loc[user_id][user_artist_matrix.loc[user_id] > 0].index.tolist()\n",
    "    \n",
    "    recommendations = {}\n",
    "    for artist in interacted_artists:\n",
    "        # Get the most similar artists to the ones the user interacted with\n",
    "        similar_artists = artist_similarity_df[artist].sort_values(ascending=False).index[1:]  # Exclude the artist itself\n",
    "\n",
    "        for similar_artist in similar_artists:\n",
    "            # Add the similar artist to recommendations with a score (using the similarity as a weight)\n",
    "            if similar_artist not in recommendations:\n",
    "                recommendations[similar_artist] = artist_similarity_df[artist][similar_artist]\n",
    "            else:\n",
    "                recommendations[similar_artist] += artist_similarity_df[artist][similar_artist]\n",
    "\n",
    "    # Sort recommendations by score (highest first)\n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Convert artist IDs to names using artist_id_to_name\n",
    "    recommended_artists = [(artist_id, artist_id_to_name.get(artist_id, \"Unknown\"), score) for artist_id, score in sorted_recommendations[:top_n]]\n",
    "\n",
    "    return recommended_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Item-Based Recommendations for User 2:\n",
      "Artist ID: 74, Artist: Basia, Similarity Score: 24.97\n",
      "Artist ID: 92, Artist: Vitamin Z, Similarity Score: 24.97\n",
      "Artist ID: 79, Artist: Fiction Factory, Similarity Score: 24.97\n",
      "Artist ID: 87, Artist: Deacon Blue, Similarity Score: 24.97\n",
      "Artist ID: 60, Artist: Matt Bianco, Similarity Score: 23.97\n",
      "Time elapsed: 4.4943 seconds\n"
     ]
    }
   ],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 item-based recommendations for user with userID=2\n",
    "user_id = 2\n",
    "item_based_recommendations = get_item_based_recommendations(user_id, user_artist_matrix, artist_similarity_df, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display item-based recommendations\n",
    "print(\"\\nTop Item-Based Recommendations for User 2:\")\n",
    "for artist_id, artist_name, score in item_based_recommendations:\n",
    "    print(f\"Artist ID: {artist_id}, Artist: {artist_name}, Similarity Score: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the item-based method gives different recommendations to the user-based method. The similarity scores for the top 5 recommendations are higher for the item-based model. We can see that the item-based method is much faster than the user-based method for this datset. This is because the user-based function iterates over both similar users and artists, whereas as the item-based method only iterates over similar artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Item-Based Recommendations for User 3:\n",
      "Artist ID: 134, Artist: Big Brotherz, Similarity Score: 41.77\n",
      "Artist ID: 131, Artist: Part Timer, Similarity Score: 41.77\n",
      "Artist ID: 130, Artist: Philippe Lamy, Similarity Score: 41.77\n",
      "Artist ID: 129, Artist: Aless, Similarity Score: 41.77\n",
      "Artist ID: 128, Artist: strom noir, Similarity Score: 41.77\n",
      "Time elapsed: 4.7258 seconds\n"
     ]
    }
   ],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 item-based recommendations for user with userID=3\n",
    "user_id = 3\n",
    "item_based_recommendations = get_item_based_recommendations(user_id, user_artist_matrix, artist_similarity_df, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display item-based recommendations\n",
    "print(\"\\nTop Item-Based Recommendations for User 3:\")\n",
    "for artist_id, artist_name, score in item_based_recommendations:\n",
    "    print(f\"Artist ID: {artist_id}, Artist: {artist_name}, Similarity Score: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see that the item-based method gives different recommendations to the user-based method. The similarity scores for the top 5 recommendations are much higher for the item-based model for user 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model-based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Singular Value Decomposition\n",
    "We compute the SVD of the user-artist matrix using `scikit-learn` and use this to give recommendations. The SVD may help to identify patterns in the data and knowledge of these could improve our receommendations. We apply a SVD model to the user-artist matrix to get the SVD components (artist features), and then approximate the original user-artist matrix. This approximation matrix is what we use to make our recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svd_recommendations(user_id, user_artist_matrix, artist_id_to_name, top_n=10, n_components=50):\n",
    "    # Apply SVD to the user-artist matrix\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    svd_matrix = svd.fit_transform(user_artist_matrix)\n",
    "    svd_components = svd.components_\n",
    "\n",
    "    # Reconstruct the user-artist interaction matrix\n",
    "    reconstructed_matrix = np.dot(svd_matrix, svd_components)\n",
    "    \n",
    "    recommendations = {}\n",
    "        \n",
    "    # Get the user's interaction vector from the reconstructed matrix\n",
    "    reconstructed_user_vector = reconstructed_matrix[user_id - 2]  # User IDs start at 2, so subtract 2\n",
    "    \n",
    "    # Iterate through all artists to recommend\n",
    "    for i, score in enumerate(reconstructed_user_vector):\n",
    "        # Check if the artist has been interacted with (score > 0) and if the artist ID is valid\n",
    "        if user_artist_matrix.iloc[user_id - 2, i] == 0:  # Ensure we only recommend non-interacted artists\n",
    "            artist_id = i  # The index of the artist in the matrix\n",
    "            if artist_id not in recommendations:\n",
    "                recommendations[artist_id] = score\n",
    "            else:\n",
    "                recommendations[artist_id] += score\n",
    "    \n",
    "    # Sort recommendations by score (highest first)\n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Convert artist IDs to names using the artist_id_to_name mapping\n",
    "    recommended_artists = [(artist_id, artist_id_to_name.get(artist_id, \"Unknown\"), score)\n",
    "                           for artist_id, score in sorted_recommendations[:top_n]]\n",
    "    \n",
    "    return recommended_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top SVD-Based Recommendations for User 2:\n",
      "Artist ID: 3464, Artist: Counting Crows, Similarity Score: 2346.16\n",
      "Artist ID: 1089, Artist: Suede, Similarity Score: 1826.24\n",
      "Artist ID: 259, Artist: 9th Wonder, Similarity Score: 1581.01\n",
      "Artist ID: 153, Artist: De/Vision, Similarity Score: 1536.43\n",
      "Artist ID: 992, Artist: Chris Rea, Similarity Score: 1110.52\n",
      "Time elapsed: 1.3591 seconds\n"
     ]
    }
   ],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 SVD-based recommendations for user with userID=2\n",
    "user_id = 2\n",
    "svd_recommendations = get_svd_recommendations(user_id, user_artist_matrix, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display SVD-based recommendations\n",
    "print(\"\\nTop SVD-Based Recommendations for User 2:\")\n",
    "for artist_id, artist_name, score in svd_recommendations:\n",
    "    print(f\"Artist ID: {artist_id}, Artist: {artist_name}, Similarity Score: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top SVD-Based Recommendations for User 3:\n",
      "Artist ID: 184, Artist: James Blunt, Similarity Score: 6.19\n",
      "Artist ID: 148, Artist: The Boats, Similarity Score: 4.82\n",
      "Artist ID: 1089, Artist: Suede, Similarity Score: 3.29\n",
      "Artist ID: 151, Artist: Deep Forest, Similarity Score: 2.92\n",
      "Artist ID: 298, Artist: Lily Allen, Similarity Score: 2.87\n",
      "Time elapsed: 1.2942 seconds\n"
     ]
    }
   ],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 SVD-based recommendations for user with userID=3\n",
    "user_id = 3\n",
    "svd_recommendations = get_svd_recommendations(user_id, user_artist_matrix, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display SVD-based recommendations\n",
    "print(\"\\nTop SVD-Based Recommendations for User 3:\")\n",
    "for artist_id, artist_name, score in svd_recommendations:\n",
    "    print(f\"Artist ID: {artist_id}, Artist: {artist_name}, Similarity Score: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommendations for user 2 had significantly higher similarity scores compared to the user-based and item-based methods from before. However, for user 3, the SVD gave higher similarities than the user-based method but lower than the item-based method. This suggests that the performance of the method is highly dependenet on the available data and that this must be considered when choosing which technique we use. So far, the SVD method is the fastest at giving recommendations, since the SVD reduces the dimensionality of the user-artist matrix by keeping only the most important features, making computations more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 ALS Using `implicit`\n",
    "A library developed for efficient recommendation systems using Python is `implicit`. We can use this with a sparse matrix of user or item weights to give recommendations. We initialise using implicit.als.AlternatingLeastSquares() and then use .fit() and .recommend() to fit our model and give recommendations.\n",
    "\n",
    "We now implement the very popular ALS method using the `implicit` library for efficiency. We use the csr_matrix() function from `scipy` to convert the user-artist matrix to a sparse format that is suitbale fort ALS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_als_recommendations(user_id, user_artist_matrix, artist_id_to_name, top_n=5, factors=50, regularization=0.1, iterations=20):\n",
    "    # Convert the user-artist matrix to sparse format (csr_matrix)\n",
    "    sparse_matrix = csr_matrix(user_artist_matrix.values)\n",
    "    \n",
    "    # Initialize and train the ALS model\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=factors, regularization=regularization, iterations=iterations)\n",
    "    model.fit(sparse_matrix)\n",
    "\n",
    "    # Get the user's interaction vector (row from sparse matrix)\n",
    "    user_vector = sparse_matrix[user_id]\n",
    "\n",
    "    # Get top N artist recommendations (returns artist IDs and scores)\n",
    "    recommendations = model.recommend(user_id, user_vector, N=top_n)\n",
    "\n",
    "    # Convert artist IDs to artist names using the provided dictionary\n",
    "    recommended_artists = [(artist_id_to_name[artist_id], score) for artist_id, score in zip(recommendations[0], recommendations[1])]\n",
    "\n",
    "    return recommended_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milse\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 8 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7df61e10bf44dfa0791982a8189ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top ALS-Based Recommendations for User 2:\n",
      "Artist: Billy Ray Cyrus, Predicted Listening Count: 1.34\n",
      "Artist: Coldplay, Predicted Listening Count: 1.30\n",
      "Artist: Kiko Loureiro, Predicted Listening Count: 1.30\n",
      "Artist: B.o.B, Predicted Listening Count: 1.29\n",
      "Artist: Сплин, Predicted Listening Count: 1.16\n",
      "Time elapsed: 1.3210 seconds\n"
     ]
    }
   ],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 ALS-based recommendations for user with userID=2\n",
    "user_id = 2\n",
    "als_recommendations = get_als_recommendations(user_id, user_artist_matrix, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display ALS-based recommendations\n",
    "print(f\"\\nTop ALS-Based Recommendations for User {user_id}:\")\n",
    "for artist_name, score in als_recommendations:\n",
    "    print(f\"Artist: {artist_name}, Predicted Listening Count: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadf76d53f4c4acab0ff98d0a0284ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top ALS-Based Recommendations for User 3:\n",
      "Artist: De/Vision, Predicted Listening Count: 1.16\n",
      "Artist: Marc Almond, Predicted Listening Count: 1.12\n",
      "Artist: Birth Control, Predicted Listening Count: 1.12\n",
      "Artist: Gothminister, Predicted Listening Count: 1.06\n",
      "Artist: Sonic Youth, Predicted Listening Count: 1.05\n",
      "Time elapsed: 1.2019 seconds\n"
     ]
    }
   ],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 ALS-based recommendations for user with userID=3\n",
    "user_id = 3\n",
    "als_recommendations = get_als_recommendations(user_id, user_artist_matrix, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display ALS-based recommendations\n",
    "print(f\"\\nTop ALS-Based Recommendations for User {user_id}:\")\n",
    "for artist_name, score in als_recommendations:\n",
    "    print(f\"Artist: {artist_name}, Predicted Listening Count: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this method returns predicted listening counts, unlike our other methods. All of the predicted listening counts for the top 5 recommendations are similar and are in the range 1.22-1.25. The artists are quite different in terms of genre. This method takes a similar amount of time as the SVD method for this dataset. This suggests that ALS is amn efficient method for giving recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Collaborative Filtering with PySpark\n",
    "Apache Spark is an engine used to process data at a large scale efficiently. It has APIs in Python and R. PySpark is the Python API for Apache Spark. PySpark has features which include Spark SQL, dataframes and machine learning.\n",
    "\n",
    "Using PySpark dataframes allows us to efficiently analyse and tranform data by using Python and Spark SQL together. Spark SQL is the Apache Spark module for using structured data, like dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 ALS Collaborative Filtering with PySpark\n",
    "We can use the `pyspark.ml` library to implement ALS. We import the ALS model from `pyspark.ml.recommendation` to create our model, then use .fit() and .recommendForAllUsers() and .recommendForAllItems() to make recommendations.\n",
    "\n",
    "We now implement ALS using PySpark methods to improve the efficiency of our recommender system, since it is highly scalable if we were to use our recommender system for very large datasets. We will try user-based and item-based implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"CollaborativeFilteringALS\").getOrCreate()\n",
    "\n",
    "# Convert cleaned pandas DataFrames to PySpark DataFrames\n",
    "artists_spark_df = spark.createDataFrame(artists_cleaned)\n",
    "user_artists_spark_df = spark.createDataFrame(user_artists_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 User-based PySpark ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# ALS model setup for user-based collaborative filtering\n",
    "# listening counts are implicit feedback, we are not starting from a cold-start\n",
    "als = ALS(userCol=\"userID\", itemCol=\"artistID\", ratingCol=\"weight\", coldStartStrategy=\"drop\", implicitPrefs=True)\n",
    "\n",
    "# Fit the ALS model\n",
    "model = als.fit(user_artists_spark_df)\n",
    "\n",
    "# Generate recommendations\n",
    "user_recommendations = model.recommendForAllUsers(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|userID|recommendations                                                                                                              |\n",
      "+------+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|3     |[{Digitalism, 1.13}, {Caribou, 1.09}, {Bondage Fairies, 1.04}, {Bang Gang, 1.01}, {Telefon Tel Aviv, 1.01}]                  |\n",
      "|5     |[{Stereophonics, 1.17}, {God Is an Astronaut, 1.13}, {The Verve, 1.12}, {Babyshambles, 1.1}, {M83, 1.08}]                    |\n",
      "|6     |[{Eminem, 0.52}, {Whitney Houston, 0.5}, {Jay-Z, 0.5}, {Brandy, 0.49}, {Hanson, 0.49}]                                       |\n",
      "|12    |[{Die Toten Hosen, 1.32}, {the GazettE, 1.31}, {Die Ärzte, 1.28}, {Guns N' Roses, 1.27}, {Scorpions, 1.26}]                  |\n",
      "|13    |[{Lady Gaga, 0.92}, {Madonna, 0.91}, {Glee Cast, 0.91}, {Avril Lavigne, 0.9}, {Michael Jackson, 0.9}]                        |\n",
      "|15    |[{Jamie Cullum, 1.25}, {Johann Sebastian Bach, 1.2}, {Mando Diao, 1.15}, {James, 1.13}, {Ella Fitzgerald, 1.12}]             |\n",
      "|16    |[{Hande Yener, 1.57}, {Dangerous Muse, 1.39}, {Ricky Martin, 1.33}, {Groove Coverage, 1.32}, {Porcelain and the Tramps, 1.3}]|\n",
      "|20    |[{DJ Shadow, 1.33}, {Burial, 1.26}, {Parov Stelar, 1.26}, {Tricky, 1.21}, {Trentemøller, 1.2}]                               |\n",
      "|22    |[{Anti-Flag, 1.38}, {Saosin, 1.34}, {From Autumn to Ashes, 1.34}, {Funeral for a Friend, 1.32}, {Bloodhound Gang, 1.29}]     |\n",
      "|26    |[{Def Leppard, 1.33}, {Mötley Crüe, 1.24}, {Joan Jett and the Blackhearts, 1.23}, {Tenacious D, 1.21}, {Poison, 1.2}]        |\n",
      "|27    |[{Red Hot Chili Peppers, 1.18}, {Queen, 1.16}, {Muse, 1.15}, {Nirvana, 1.15}, {Elvis Presley, 1.15}]                         |\n",
      "|28    |[{Orchestral Manoeuvres in the Dark, 0.36}, {Erasure, 0.35}, {Camouflage, 0.34}, {The Human League, 0.34}, {Ultravox, 0.34}] |\n",
      "|31    |[{UNKLE, 1.16}, {Enigma, 1.15}, {a-ha, 1.15}, {Aqua, 1.14}, {Steve Jablonsky, 1.08}]                                         |\n",
      "|34    |[{Rufus Wainwright, 1.37}, {Eels, 1.24}, {Bruce Springsteen, 1.24}, {Sex Pistols, 1.23}, {Janis Joplin, 1.21}]               |\n",
      "|40    |[{Animal ДжаZ, 1.23}, {Tricky, 1.22}, {Steve Jablonsky, 1.22}, {Institute, 1.2}, {VAST, 1.18}]                               |\n",
      "|41    |[{Journey, 1.41}, {Def Leppard, 1.19}, {ZZ Top, 1.15}, {Poison, 1.15}, {Van Halen, 1.14}]                                    |\n",
      "|43    |[{Kanye West, 1.15}, {Eminem, 1.09}, {Sneaker Pimps, 1.08}, {Circa Survive, 1.08}, {Fatboy Slim, 1.06}]                      |\n",
      "|44    |[{John Frusciante, 1.1}, {Bob Marley, 1.09}, {Misfits, 1.08}, {Bob Dylan, 1.07}, {The Clash, 1.05}]                          |\n",
      "|47    |[{Gwen Stefani, 1.02}, {Ne-Yo, 1.02}, {Fergie, 1.02}, {Adam Lambert, 1.02}, {B.o.B, 1.02}]                                   |\n",
      "|48    |[{Legião Urbana, 1.08}, {Tarja, 1.07}, {Cyndi Lauper, 1.04}, {Bryan Adams, 1.02}, {Michael Bublé, 1.02}]                     |\n",
      "+------+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time elapsed: 35.5539 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to map artistID to artistName\n",
    "artist_id_to_name = {row['id']: row['name'] for row in artists_spark_df.collect()}\n",
    "\n",
    "# Function to map artistID to artistName and round scores to 2 decimal places\n",
    "def map_recommendations(user_recommendations):\n",
    "    def map_row(row):\n",
    "        recommendations_with_names = [\n",
    "            (artist_id_to_name.get(rec[0], \"Unknown\"), round(rec[1], 2)) for rec in row['recommendations']\n",
    "        ]\n",
    "        return (row['userID'], recommendations_with_names)\n",
    "\n",
    "    mapped_recommendations = user_recommendations.rdd.map(map_row).toDF([\"userID\", \"recommendations\"])\n",
    "    return mapped_recommendations\n",
    "\n",
    "# Apply the artistID to name mapping function\n",
    "user_recommendations_with_names = map_recommendations(user_recommendations)\n",
    "\n",
    "# Show the final recommendations with artist names and rounded scores\n",
    "user_recommendations_with_names.show(truncate=False)\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the output is recommendations for all users and this was computed very quickly, so the use of PySpark is effective. Note that the given time is much longer than the previous methods, but this method has givemn recommendations for all users, not just one user at a time. This clearly shows the efficiency and scalability of PySpark ALS for recommender systems.\n",
    "\n",
    "Also, the output gives recommendations with a 'score'. This is the relative confidence that a user will like a given artist. Clearly, some users, like user 16, have higher score but some users, like user 28, have much lower scores. This would indicate that the recommendations for user 16 relative to user 28 are much better. This is likely dependent on the availability of data for the different users.\n",
    "\n",
    "We will analyse the distribution of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             score|\n",
      "+-------+------------------+\n",
      "|  count|              9460|\n",
      "|   mean| 1.107177987835177|\n",
      "| stddev|0.2619573931045368|\n",
      "|    min|       4.51211E-40|\n",
      "|    25%|         1.0323541|\n",
      "|    50%|          1.120324|\n",
      "|    75%|         1.2466799|\n",
      "|    max|          2.111733|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode the recommendations column into individual rows\n",
    "exploded_user_recommendations = user_recommendations.withColumn(\"recommendation\", F.explode(\"recommendations\"))\n",
    "\n",
    "# Extract artistID and rating\n",
    "exploded_user_recommendations = exploded_user_recommendations.select(\n",
    "    F.col(\"userID\"),\n",
    "    F.col(\"recommendation.artistID\").alias(\"artistID\"),  # Extract artistID\n",
    "    F.col(\"recommendation.rating\").alias(\"score\")        # Extract rating as score\n",
    ")\n",
    "\n",
    "# Generate summary statistics for the scores\n",
    "summary_stats = exploded_user_recommendations.select(\"score\").summary()\n",
    "\n",
    "# Display the summary statistics\n",
    "summary_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the scores range from around 0 to 2, with most of the recommendations being between 1.03 and 1.25. We then will say that the best recommendations are given by scores above 1.25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Item-based PySpark ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# ALS model setup for item-based collaborative filtering\n",
    "# Swap userCol and itemCol for item-based filtering\n",
    "# listening counts are implicit feedback, we are not starting from a cold-start\n",
    "als = ALS(userCol=\"artistID\", itemCol=\"userID\", ratingCol=\"weight\", coldStartStrategy=\"drop\", implicitPrefs=True)\n",
    "\n",
    "# Fit the ALS model\n",
    "model = als.fit(user_artists_spark_df)\n",
    "\n",
    "# Generate item-based recommendations for each artist (item)\n",
    "item_recommendations = model.recommendForAllItems(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|userID|recommendations                                                                                                            |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|3     |[{梶浦由記, 1.38}, {Télépopmusik, 1.26}, {Flunk, 1.18}, {Camille, 1.03}, {Slimmy, 1.03}]                                   |\n",
      "|5     |[{Jamie Cullum, 1.11}, {Morcheeba, 1.1}, {Radiohead, 1.08}, {Beirut, 1.07}, {The Smiths, 1.07}]                            |\n",
      "|6     |[{Mary J. Blige, 0.48}, {Brandy, 0.48}, {Scissor Sisters, 0.48}, {Toni Braxton, 0.48}, {Jay-Z, 0.47}]                      |\n",
      "|12    |[{Eminem, 1.3}, {Lil' Wayne, 1.29}, {Drake, 1.29}, {The Faint, 1.26}, {Armin van Buuren, 1.26}]                            |\n",
      "|13    |[{Madonna, 0.94}, {Lady Gaga, 0.93}, {Rihanna, 0.91}, {Britney Spears, 0.9}, {Glee Cast, 0.9}]                             |\n",
      "|15    |[{Morcheeba, 1.3}, {T. Rex, 1.28}, {Jamie Cullum, 1.24}, {They Might Be Giants, 1.22}, {Ólafur Arnalds, 1.21}]             |\n",
      "|16    |[{Dolores O'Riordan, 1.5}, {Ricky Martin, 1.48}, {La Quinta Estación, 1.4}, {Delta Goodrem, 1.38}, {Juanes, 1.38}]         |\n",
      "|20    |[{Gang Starr, 1.2}, {Bondage Fairies, 1.2}, {deadmau5, 1.19}, {MF DOOM, 1.19}, {The Dillinger Escape Plan, 1.18}]          |\n",
      "|22    |[{The Agonist, 1.33}, {Coheed and Cambria, 1.24}, {Volbeat, 1.22}, {Sodom, 1.21}, {DevilDriver, 1.2}]                      |\n",
      "|26    |[{In Extremo, 1.18}, {Yanni, 1.17}, {Manowar, 1.16}, {Mägo de Oz, 1.16}, {Agalloch, 1.14}]                                 |\n",
      "|27    |[{Coldplay, 1.2}, {The Killers, 1.19}, {Placebo, 1.19}, {The Beatles, 1.17}, {Red Hot Chili Peppers, 1.17}]                |\n",
      "|28    |[{Orchestral Manoeuvres in the Dark, 0.34}, {Tears for Fears, 0.33}, {Soft Cell, 0.33}, {Eurythmics, 0.33}, {Recoil, 0.33}]|\n",
      "|31    |[{George Michael, 1.09}, {Erasure, 1.09}, {Madonna, 1.07}, {Recoil, 1.07}, {Amy Macdonald, 1.07}]                          |\n",
      "|34    |[{Ólafur Arnalds, 1.31}, {Jamie Cullum, 1.27}, {Morcheeba, 1.25}, {Wolfgang Amadeus Mozart, 1.24}, {Mike Oldfield, 1.23}]  |\n",
      "|40    |[{Diorama, 1.32}, {KMFDM, 1.3}, {The Presets, 1.22}, {Combichrist, 1.18}, {Ayria, 1.16}]                                   |\n",
      "|41    |[{In Extremo, 1.33}, {Lordi, 1.23}, {Wir sind Helden, 1.19}, {Siouxsie and the Banshees, 1.18}, {Dimmu Borgir, 1.17}]      |\n",
      "|43    |[{Kanye West, 1.15}, {ATB, 1.12}, {Daft Punk, 1.08}, {Pendulum, 1.06}, {Madonna, 1.05}]                                    |\n",
      "|44    |[{Bob Dylan, 1.09}, {T. Rex, 1.08}, {The Clash, 1.08}, {Pink Floyd, 1.07}, {The Doors, 1.07}]                              |\n",
      "|47    |[{Janet Jackson, 1.06}, {Adam Lambert, 1.03}, {Taylor Swift, 1.03}, {Avril Lavigne, 1.03}, {Glee Cast, 1.03}]              |\n",
      "|48    |[{Bee Gees, 1.11}, {Cyndi Lauper, 1.06}, {Amy Macdonald, 1.04}, {Blondie, 1.03}, {ABBA, 1.03}]                             |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time elapsed: 23.8395 seconds\n"
     ]
    }
   ],
   "source": [
    "# Apply the artistID to name mapping function\n",
    "item_recommendations_with_names = map_recommendations(item_recommendations)\n",
    "\n",
    "# Show the final recommendations with artist names and rounded scores\n",
    "item_recommendations_with_names.show(truncate=False)\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the item-based implementation seems to be more efficient than the user-based due to the different iterations needed by each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              score|\n",
      "+-------+-------------------+\n",
      "|  count|               9460|\n",
      "|   mean|  1.110033392048752|\n",
      "| stddev|0.26660361888926437|\n",
      "|    min|      1.4103748E-38|\n",
      "|    25%|          1.0318737|\n",
      "|    50%|          1.1213707|\n",
      "|    75%|          1.2492609|\n",
      "|    max|          2.0857654|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode the recommendations column into individual rows\n",
    "exploded_item_recommendations = item_recommendations.withColumn(\"recommendation\", F.explode(\"recommendations\"))\n",
    "\n",
    "# Extract artistID and rating from the struct fields\n",
    "exploded_item_recommendations = exploded_item_recommendations.select(\n",
    "    F.col(\"userID\"),\n",
    "    F.col(\"recommendation.artistID\").alias(\"artistID\"),  # Extract artistID\n",
    "    F.col(\"recommendation.rating\").alias(\"score\")        # Extract rating as score\n",
    ")\n",
    "\n",
    "# Generate summary statistics for the scores\n",
    "summary_stats = exploded_item_recommendations.select(\"score\").summary()\n",
    "\n",
    "# Display the summary statistics\n",
    "summary_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores for the item-based implementation have a similar distribution to the user-based implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Conclusion\n",
    "We have investigated collaborative filtering through memory-based and model-based methods. First, we implemented simple user-based and item-based methods which were slow, but successfully gave recommendations. The item-based methods seem to run quicker than the user-based methods due to the structure of the data. We then implemented model-based methods - SVD and ALS - which were much more efficient than the previous memory-based methods, and allowed quick recommendations to be given.\n",
    "\n",
    "All of these methods gave recommendations which ranged in quality and performance depending on the availability of data. Finally, we used PySpark to implement ALS and this increased the computational efficiency significantly, allowing us to quickly give recommendations for all of the users. This feature would allow us to scale our recommendation methods to large datasets and would be vital in developing our music recommendation system, which could theoretically have millions of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**\n",
    "[1] F.O. Isinkaye, Y.O. Folajimi, B.A. Ojokoh,\n",
    "Recommendation systems: Principles, methods and evaluation,\n",
    "Egyptian Informatics Journal,\n",
    "Volume 16, Issue 3,\n",
    "2015,\n",
    "Pages 261-273.\n",
    "(https://www.sciencedirect.com/science/article/pii/S1110866515000341)\n",
    "\n",
    "[2] Implicit Documentation: https://benfred.github.io/implicit/\n",
    "\n",
    "[3] PySpark Collaborative Filtering Documentation: https://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n",
    "\n",
    "[4] scikit-learn Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "\n",
    "[5] csr_matrix Documentation: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    " \n",
    "[6] PySpark ALS Documentation: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.recommendation.ALS.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
