{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I am aiming to improve the efficiency and performance of my VAE model in VAE2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv('../unpushed_work/last_fm_data/user_artists.dat', sep='\\t')\n",
    "\n",
    "user_interaction_counts = user_data.groupby('userID').size()\n",
    "users_with_50_interactions = user_interaction_counts[user_interaction_counts >= 50].index\n",
    "user_data_filtered = user_data[user_data['userID'].isin(users_with_50_interactions)]\n",
    "\n",
    "unique_users = user_data_filtered['userID'].unique()\n",
    "\n",
    "# Ensure that test users have at least 50 interactions in 'test_data'\n",
    "# We need to carefully select 'test_users' to satisfy this condition\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "shuffled_users = np.random.permutation(unique_users)\n",
    "\n",
    "train_users = []\n",
    "test_users = []\n",
    "\n",
    "# We'll collect test users until we have enough that have at least 50 interactions\n",
    "for user in shuffled_users:\n",
    "    user_data_temp = user_data_filtered[user_data_filtered['userID'] == user]\n",
    "    if len(test_users) < int(0.2 * len(unique_users)):\n",
    "        # Tentatively add to test_users\n",
    "        test_users.append(user)\n",
    "    else:\n",
    "        train_users.append(user)\n",
    "\n",
    "# Recreate test_data and train_data\n",
    "train_data = user_data_filtered[user_data_filtered['userID'].isin(train_users)]\n",
    "test_data = user_data_filtered[user_data_filtered['userID'].isin(test_users)]\n",
    "\n",
    "# Now check that each user in test_data has 50 interactions\n",
    "# Remove any users from test_users who don't meet this criterion\n",
    "valid_test_users = []\n",
    "for user in test_users:\n",
    "    user_data_temp = test_data[test_data['userID'] == user]\n",
    "    if len(user_data_temp) == 50:\n",
    "        valid_test_users.append(user)\n",
    "\n",
    "# Update test_users and test_data\n",
    "test_users = valid_test_users\n",
    "test_data = test_data[test_data['userID'].isin(test_users)]\n",
    "\n",
    "# Update train_data to include any users removed from test_users\n",
    "removed_test_users = set(shuffled_users) - set(train_users) - set(test_users)\n",
    "if removed_test_users:\n",
    "    train_users.extend(list(removed_test_users))\n",
    "    train_data = user_data_filtered[user_data_filtered['userID'].isin(train_users)]\n",
    "\n",
    "# Now proceed to split test_data into test_x and test_y\n",
    "test_x = pd.DataFrame(columns=test_data.columns)\n",
    "test_y = pd.DataFrame(columns=test_data.columns)\n",
    "\n",
    "for user in test_users:\n",
    "    user_data_temp = test_data[test_data['userID'] == user]\n",
    "    user_data_shuffled = user_data_temp.sample(frac=1, random_state=42)\n",
    "    user_test_x = user_data_shuffled.iloc[:25]\n",
    "    user_test_y = user_data_shuffled.iloc[25:50]\n",
    "    test_x = pd.concat([test_x, user_test_x], ignore_index=True)\n",
    "    test_y = pd.concat([test_y, user_test_y], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode artistIDs\n",
    "# Essential since ML models require numerical input (also efficient)\n",
    "artist_encoder = LabelEncoder()\n",
    "all_artistIDs = user_data_filtered['artistID'].unique()\n",
    "artist_encoder.fit(all_artistIDs)\n",
    "num_artists = len(artist_encoder.classes_)\n",
    "\n",
    "# Encode train users\n",
    "train_user_encoder = LabelEncoder()\n",
    "train_user_encoder.fit(train_users)\n",
    "num_train_users = len(train_user_encoder.classes_)\n",
    "\n",
    "# Encode test users\n",
    "test_user_encoder = LabelEncoder()\n",
    "test_user_encoder.fit(test_users)\n",
    "num_test_users = len(test_user_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create user-item interaction matrix\n",
    "def create_user_item_matrix(data, user_encoder, num_users):\n",
    "    user_item_matrix = np.zeros((num_users, num_artists))\n",
    "    for idx, row in data.iterrows():\n",
    "        user_idx = user_encoder.transform([row['userID']])[0]\n",
    "        artist_idx = artist_encoder.transform([row['artistID']])[0]\n",
    "        weight = row['weight']\n",
    "        user_item_matrix[user_idx, artist_idx] = weight\n",
    "    return user_item_matrix\n",
    "\n",
    "train_user_item_matrix = create_user_item_matrix(train_data, train_user_encoder, num_train_users)\n",
    "test_x_user_item_matrix = create_user_item_matrix(test_x, test_user_encoder, num_test_users)\n",
    "test_y_user_item_matrix = create_user_item_matrix(test_y, test_user_encoder, num_test_users)\n",
    "\n",
    "# Scale data before inputting it into autoencoder\n",
    "# Find the maximum and minimum values across all matrices for consistent scaling\n",
    "max_value = max(train_user_item_matrix.max(), test_x_user_item_matrix.max(), test_y_user_item_matrix.max())\n",
    "min_value = min(train_user_item_matrix.min(), test_x_user_item_matrix.min(), test_y_user_item_matrix.min())\n",
    "\n",
    "# Define a function to normalize a matrix\n",
    "def normalise(matrix, min_value, max_value):\n",
    "    return (matrix - min_value) / (max_value - min_value)\n",
    "\n",
    "# Normalize each matrix\n",
    "train_user_item_matrix = normalise(train_user_item_matrix, min_value, max_value)\n",
    "test_x_user_item_matrix = normalise(test_x_user_item_matrix, min_value, max_value)\n",
    "test_y_user_item_matrix = normalise(test_y_user_item_matrix, min_value, max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterise(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std) # Adds noise by sampling from standard normal dist.\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.fc_decode(z))\n",
    "        return torch.sigmoid(self.fc_out(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2228.4047\n",
      "Epoch 2, Loss: 226.2412\n",
      "Epoch 3, Loss: 21.8895\n",
      "Epoch 4, Loss: 11.3289\n",
      "Epoch 5, Loss: 10.5715\n",
      "Epoch 6, Loss: 9.8042\n",
      "Epoch 7, Loss: 9.2067\n",
      "Epoch 8, Loss: 8.5201\n",
      "Epoch 9, Loss: 8.2623\n",
      "Epoch 10, Loss: 7.4779\n"
     ]
    }
   ],
   "source": [
    "# Convert train data to tensor\n",
    "train_tensor = torch.FloatTensor(train_user_item_matrix)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "train_dataset = torch.utils.data.TensorDataset(train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "input_dim = num_artists\n",
    "hidden_dim = 256\n",
    "latent_dim = 50\n",
    "\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data_batch in train_loader:\n",
    "        data = data_batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {train_loss / len(train_loader.dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.0002\n"
     ]
    }
   ],
   "source": [
    "# Convert test data to tensor\n",
    "test_x_tensor = torch.FloatTensor(test_x_user_item_matrix)\n",
    "test_y_tensor = torch.FloatTensor(test_y_user_item_matrix)\n",
    "\n",
    "# Get the model's predictions on test_x\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions, _, _ = model(test_x_tensor) # Reconstructed predictions\n",
    "\n",
    "mse_loss = nn.functional.mse_loss(predictions, test_y_tensor)\n",
    "print(f\"Test MSE: {mse_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnormalised test MSE: 19942642.0000\n"
     ]
    }
   ],
   "source": [
    "# Instead of using MSE, we need a better way to evaluate our model. Since the entries of our matrices are normalised (i.e. between 0 and 1), the MSE will\n",
    "# always be small, potentially leading to false confidence in our model.\n",
    "# First we will unnormalise the matrices and then find the MSE\n",
    "\n",
    "def unnormalise(matrix, min_value, max_value):\n",
    "    return matrix * (max_value - min_value) + min_value\n",
    "\n",
    "predictions_not_normal = unnormalise(predictions, min_value, max_value)\n",
    "test_y_tensor_not_normal = unnormalise(test_y_tensor, min_value, max_value)\n",
    "\n",
    "mse_loss = nn.functional.mse_loss(predictions_not_normal, test_y_tensor_not_normal)\n",
    "print(f\"Unnormalised test MSE: {mse_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, this MSE is extremely high, however it is not necessarily reflective of the predictive power of our model. \n",
    "# We will now try the Hit Rate@k (HR@k) as a metric, which measures the fraction of users for which the recommender system successfully recomends at least one\n",
    "# relevant item within the top-k recommendations\n",
    "\n",
    "def hit_rate_at_k(true_matrix, predicted_matrix, k):\n",
    "    hits = 0\n",
    "    n_users = true_matrix.shape[0]\n",
    "\n",
    "    for user_idx in range(n_users):\n",
    "\n",
    "        _, top_k_indices = torch.topk(predicted_matrix[user_idx], k=k, largest=True, sorted=True)\n",
    "\n",
    "        if torch.any(true_matrix[user_idx][top_k_indices] > 0):\n",
    "            hits += 1\n",
    "\n",
    "    return hits / n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate for k=5 : 0.005479452054794521\n",
      "Hit Rate for k=7 : 0.00821917808219178\n",
      "Hit Rate for k=10 : 0.0136986301369863\n",
      "Hit Rate for k=20 : 0.03561643835616438\n",
      "Hit Rate for k=50 : 0.057534246575342465\n",
      "Hit Rate for k=100 : 0.12054794520547946\n",
      "Hit Rate for k=300 : 0.3835616438356164\n",
      "Hit Rate for k=500 : 0.5178082191780822\n"
     ]
    }
   ],
   "source": [
    "print(f'Hit Rate for k=5 : {hit_rate_at_k(test_y_tensor, predictions, k=5)}')\n",
    "print(f'Hit Rate for k=7 : {hit_rate_at_k(test_y_tensor, predictions, k=7)}')\n",
    "print(f'Hit Rate for k=10 : {hit_rate_at_k(test_y_tensor, predictions, k=10)}')\n",
    "print(f'Hit Rate for k=20 : {hit_rate_at_k(test_y_tensor, predictions, k=20)}')\n",
    "print(f'Hit Rate for k=50 : {hit_rate_at_k(test_y_tensor, predictions, k=50)}')\n",
    "print(f'Hit Rate for k=100 : {hit_rate_at_k(test_y_tensor, predictions, k=100)}')\n",
    "print(f'Hit Rate for k=300 : {hit_rate_at_k(test_y_tensor, predictions, k=300)}')\n",
    "print(f'Hit Rate for k=500 : {hit_rate_at_k(test_y_tensor, predictions, k=500)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
