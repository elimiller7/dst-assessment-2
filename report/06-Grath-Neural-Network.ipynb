{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Model\n",
    "\n",
    "We have already laid out the basic idea of Graph Neural Networks (GNNs) in the introduction, and as such we will instead introduce a useful variant, known as Graph Convolutional Networks (GCNs). GCNs are the most prevelant and broadly applied model of GNNs, [1] which are able to leverage both the information of the nodes and their locality to make predictions [2]. They do this using a convolution layer, which merges the features of a node with those of its neighbours. The most common method of recommender systems used with GCNs is Collaborative Filtering, where user's past interactions and connections to other users is used to make predictions. The main way it does this is through learning latent features (or an embedding) for each user and item node. GCNs then update this embedding by propagating information across the graph:\n",
    "\n",
    "Let $\\mathbf{e}_u^{(0)}$ denote the user embedding of user $u$, and $\\mathbf{e}_i^{(0)}$ the item embedding of item $i$. Then after $k$ layers of propagation, we have:\n",
    "\n",
    "$$\\mathbf{e}_u^{(k+1)} = \\sigma \\left( \\mathbf{W}_1 \\mathbf{e}_u^{(k)} + \\sum_{i\\in \\mathcal{N}_u} \\frac{1}{\\sqrt{|\\mathcal{N}_u||\\mathcal{N}_i|}} (\\mathbf{W}_1\\mathbf{e}_u^{(k)} + \\mathbf{W}_2(\\mathbf{e}_i^{(k)}\\odot \\mathbf{e}_u^{(k)}))\\right)$$\n",
    "\n",
    "and a similar equation for $\\mathbf{e}_i^{(k+1)}$ [3]. In this equation, $\\sigma$ is the nonlinear activation function (e.g. ReLU), $\\mathcal{N}_u$ denotes the set of items that are interacted with by user $u$, and similar for $\\mathcal{N}_i$ and item $i$. $\\mathbf{W}_1$ and $\\mathbf{W}_2$ are trainable weight matrices used to perform feature transformations in each layer.\n",
    "\n",
    "These equations represent the basic building block of most GCNs, however after recent investigations as published in paper [3], it was discovered that for recommendation systems in particular, both the activation function, $\\sigma$, and the feature transformation matrices, $\\mathbf{W}_1, \\mathbf{W}_2$, have little influence on the model and in fact make the predictions worse in some situations. As such a new model was suggested that stripped these parts for a much simpler propogation method:\n",
    "\n",
    "$$\\mathbf{e}_u^{(k+1)} = \\sum_{i\\in \\mathcal{N}_u} \\frac{1}{\\sqrt{|\\mathcal{N}_u||\\mathcal{N}_i|}} \\mathbf{e}_i^{(k)}$$\n",
    "\n",
    "$$\\mathbf{e}_i^{(k+1)} = \\sum_{u\\in \\mathcal{N}_i} \\frac{1}{\\sqrt{|\\mathcal{N}_i||\\mathcal{N}_u|}} \\mathbf{e}_u^{(k)}$$\n",
    "\n",
    "This model is known as LightGCN, and was shown to perform better in most cases than the other prevalent GCN and GNN models for recommendation systems. This goes to show that more complicated is not always better, which carries over to data at scale. Sometimes one of the best ways to deal with big data is to make sure your model isn't performing unnecessary calculations.\n",
    "\n",
    "LightGCN is still among the best models for collaborative filtering in recommender systems, and as such is the one that we will consider here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules and Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(os.path.join('.','scripts'))\n",
    "\n",
    "import GraphFuncs as gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = pd.read_csv(os.path.join(cwd,'data','artists.dat'), delimiter='\\t')\n",
    "tags = pd.read_csv(os.path.join(cwd,'data','tags.dat'), delimiter='\\t',encoding='ISO-8859-1')\n",
    "user_artists = pd.read_csv(os.path.join(cwd,'data','user_artists.dat'), delimiter='\\t')\n",
    "user_friends = pd.read_csv(os.path.join(cwd,'data','user_friends.dat'), delimiter='\\t')\n",
    "user_taggedartists_timestamps = pd.read_csv(os.path.join(cwd,'data','user_taggedartists-timestamps.dat'), delimiter='\\t')\n",
    "user_taggedartists = pd.read_csv(os.path.join(cwd,'data','user_taggedartists.dat'), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding the Question\n",
    "\n",
    "The goal of a recommender system is to try to predict which items a user will like, and then give them recommendations based on this. In this section, for our dataset, we will look at the goal of predicting whether or not a user will like an artist. To achieve this, we will create an 80-20 training-testing split of the data. We have seen that for most users, the `user_artist` dataset contains their top 50 artists, so we will take 40 of them for each user at random, and isolate the other 10 to use as testing data. We will then train a GNN on the 40 artists in the training dataset, and evaluate the model by seeing how it predicts if the user will like the 10 artists in the testing data. If the model performs well on this target, then we could use the model to predict whether or not a user will like an artist not contained in their 50 artists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Train-Test Split\n",
    "\n",
    "In order to create the train-test split we will use the Scikit-learn module, which has a built in function for it. It also has a built in `stratify` parameter, which allows us to specify that the data should be split whilst keeping the proportion of data the same for each user, i.e. what we described above. However this requires that each user has atleast 2 artist data entries, but we see that this is not the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112, 615, 1013, 1307, 1603, 1731, 1758, 2085]\n"
     ]
    }
   ],
   "source": [
    "users = user_artists['userID'].unique()\n",
    "singleartistusers = [int(user) for user in users if len(gf.get_artists(user,user_artists)) == 1]\n",
    "print(singleartistusers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 8 users with only 1 artist data entry. As such we remove those, create the train-test split, and then re-add those users back to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the rows of data for the users which only have one artist data point.\n",
    "singleartistusersdf = user_artists[user_artists['userID'].isin(singleartistusers)]\n",
    "# Take the rest of the data into a temp df\n",
    "user_artists_temp = user_artists[~user_artists['userID'].isin(singleartistusers)]\n",
    "\n",
    "# Create the train-test split using this temp df\n",
    "user_artists_train, user_artists_test = train_test_split(user_artists_temp,\n",
    "                                                          test_size = 0.2, \n",
    "                                                          stratify = user_artists_temp['userID'], \n",
    "                                                          random_state = 47)\n",
    "\n",
    "# Concat the dfs back together\n",
    "user_artists_train = pd.concat([user_artists_train,singleartistusersdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we save the datasets as text files so that the model can access them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [user_artists_train, user_artists_test, user_friends]\n",
    "\n",
    "filepath = os.path.join(cwd,'data','GNN')\n",
    "if not os.path.exists(filepath):\n",
    "    os.makedirs(filepath)\n",
    "\n",
    "for df in dfs:\n",
    "    df.to_csv(os.path.join(filepath,gf.get_df_name(df, globals()) + '.txt'),sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Model\n",
    "\n",
    "In order to use the LightGCN model, we have downloaded the scripts from the GitHub repository [4] associated with the paper [3] that suggested the model. This repository already had a Last.fm dataset built in, however it is different to ours and as such we must alter the code to run to our needs. This mostly involved adding a new class in the `dataloader.py` script, known as LastFM2, which was done by altering the already existing LastFM class.\n",
    "\n",
    "The downloaded scripts are contained in the `scripts/LightGCN` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LastFM2(BasicDataset):\n",
      "    \"\"\"\n",
      "    Dataset type for pytorch\n",
      "    LastFM dataset 2\n",
      "    \"\"\"\n",
      "    def __init__(self, path=os.path.join(cwd,'data','GNN')):\n",
      "        # train or test\n",
      "        cprint(\"loading [last fm 2]\")\n",
      "        self.mode_dict = {'train':0, \"test\":1}\n",
      "        self.mode    = self.mode_dict['train']\n",
      "        # self.n_users = 2100\n",
      "        # self.m_items = 18745\n",
      "        trainData = pd.read_table(join(path, 'user_artists_train.txt'), header=None)\n",
      "        print(trainData.head())\n",
      "        testData  = pd.read_table(join(path, 'user_artists_test.txt'), header=None)\n",
      "        print(testData.head())\n",
      "        trustNet  = pd.read_table(join(path, 'user_friends.txt'), header=None).to_numpy()\n",
      "        print(trustNet[:5])\n",
      "        trustNet -= 1\n",
      "        trainData-= 1\n",
      "        testData -= 1\n",
      "        self.trustNet  = trustNet\n",
      "        self.trainData = trainData\n",
      "        self.testData  = testData\n",
      "        self.trainUser = np.array(trainData[:][0])\n",
      "        self.trainUniqueUsers = np.unique(self.trainUser)\n",
      "        self.trainItem = np.array(trainData[:][1])\n",
      "        # self.trainDataSize = len(self.trainUser)\n",
      "        self.testUser  = np.array(testData[:][0])\n",
      "        self.testUniqueUsers = np.unique(self.testUser)\n",
      "        self.testItem  = np.array(testData[:][1])\n",
      "        self.Graph = None\n",
      "        print(f\"LastFm2 Sparsity : {(len(self.trainUser) + len(self.testUser))/self.n_users/self.m_items}\")\n",
      "        \n",
      "        # (users,users)\n",
      "        self.socialNet    = csr_matrix((np.ones(len(trustNet)), (trustNet[:,0], trustNet[:,1]) ), shape=(self.n_users,self.n_users))\n",
      "        # (users,items), bipartite graph\n",
      "        self.UserItemNet  = csr_matrix((np.ones(len(self.trainUser)), (self.trainUser, self.trainItem) ), shape=(self.n_users,self.m_items)) \n",
      "        \n",
      "        # pre-calculate\n",
      "        self._allPos = self.getUserPosItems(list(range(self.n_users)))\n",
      "        self.allNeg = []\n",
      "        allItems    = set(range(self.m_items))\n",
      "        for i in range(self.n_users):\n",
      "            pos = set(self._allPos[i])\n",
      "            neg = allItems - pos\n",
      "            self.allNeg.append(np.array(list(neg)))\n",
      "        self.__testDict = self.__build_test()\n",
      "\n",
      "    @property\n",
      "    def n_users(self):\n",
      "        return 2100\n",
      "    \n",
      "    @property\n",
      "    def m_items(self):\n",
      "        return 18745\n",
      "    \n",
      "    @property\n",
      "    def trainDataSize(self):\n",
      "        return len(self.trainUser)\n",
      "    \n",
      "    @property\n",
      "    def testDict(self):\n",
      "        return self.__testDict\n",
      "\n",
      "    @property\n",
      "    def allPos(self):\n",
      "        return self._allPos\n",
      "\n",
      "    def getSparseGraph(self):\n",
      "        if self.Graph is None:\n",
      "            user_dim = torch.LongTensor(self.trainUser)\n",
      "            item_dim = torch.LongTensor(self.trainItem)\n",
      "            \n",
      "            first_sub = torch.stack([user_dim, item_dim + self.n_users])\n",
      "            second_sub = torch.stack([item_dim+self.n_users, user_dim])\n",
      "            index = torch.cat([first_sub, second_sub], dim=1)\n",
      "            data = torch.ones(index.size(-1)).int()\n",
      "            self.Graph = torch.sparse.IntTensor(index, data, torch.Size([self.n_users+self.m_items, self.n_users+self.m_items]))\n",
      "            dense = self.Graph.to_dense()\n",
      "            D = torch.sum(dense, dim=1).float()\n",
      "            D[D==0.] = 1.\n",
      "            D_sqrt = torch.sqrt(D).unsqueeze(dim=0)\n",
      "            dense = dense/D_sqrt\n",
      "            dense = dense/D_sqrt.t()\n",
      "            index = dense.nonzero()\n",
      "            data  = dense[dense >= 1e-9]\n",
      "            assert len(index) == len(data)\n",
      "            self.Graph = torch.sparse.FloatTensor(index.t(), data, torch.Size([self.n_users+self.m_items, self.n_users+self.m_items]))\n",
      "            self.Graph = self.Graph.coalesce().to(world.device)\n",
      "        return self.Graph\n",
      "\n",
      "    def __build_test(self):\n",
      "        \"\"\"\n",
      "        return:\n",
      "            dict: {user: [items]}\n",
      "        \"\"\"\n",
      "        test_data = {}\n",
      "        for i, item in enumerate(self.testItem):\n",
      "            user = self.testUser[i]\n",
      "            if test_data.get(user):\n",
      "                test_data[user].append(item)\n",
      "            else:\n",
      "                test_data[user] = [item]\n",
      "        return test_data\n",
      "    \n",
      "    def getUserItemFeedback(self, users, items):\n",
      "        \"\"\"\n",
      "        users:\n",
      "            shape [-1]\n",
      "        items:\n",
      "            shape [-1]\n",
      "        return:\n",
      "            feedback [-1]\n",
      "        \"\"\"\n",
      "        # print(self.UserItemNet[users, items])\n",
      "        return np.array(self.UserItemNet[users, items]).astype('uint8').reshape((-1, ))\n",
      "    \n",
      "    def getUserPosItems(self, users):\n",
      "        posItems = []\n",
      "        for user in users:\n",
      "            posItems.append(self.UserItemNet[user].nonzero()[1])\n",
      "        return posItems\n",
      "    \n",
      "    def getUserNegItems(self, users):\n",
      "        negItems = []\n",
      "        for user in users:\n",
      "            negItems.append(self.allNeg[user])\n",
      "        return negItems\n",
      "            \n",
      "    def __getitem__(self, index):\n",
      "        user = self.trainUniqueUsers[index]\n",
      "        # return user_id and the positive items of the user\n",
      "        return user\n",
      "    \n",
      "    def switch2test(self):\n",
      "        \"\"\"\n",
      "        change dataset mode to offer test data to dataloader\n",
      "        \"\"\"\n",
      "        self.mode = self.mode_dict['test']\n",
      "    \n",
      "    def __len__(self):\n",
      "        return len(self.trainUniqueUsers)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(cwd,'scripts','LightGCN','dataloader.py')) as file:\n",
    "    content = file.read()\n",
    "    start = content.index('class LastFM2')\n",
    "    print(content[start:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also made other alterations to the code to suit our needs for the model. These changes will be annotated in the scripts themself where appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Papers With Code: Graph Models - https://paperswithcode.com/methods/category/graph-models\n",
    "\n",
    "[2] Towards Data Science: Graph Convolutional Networks - https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95\n",
    "\n",
    "[3] Xiangnan He, et al. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation - https://arxiv.org/abs/2002.02126\n",
    "\n",
    "[4] Guyse1234 Github Repository: LightGCN-PyTorch - https://github.com/gusye1234/LightGCN-PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
