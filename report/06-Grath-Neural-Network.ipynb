{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to GCNs\n",
    "\n",
    "We have already laid out the basic idea of Graph Neural Networks (GNNs) in the introduction, and as such we will instead introduce a useful variant, known as Graph Convolutional Networks (GCNs). GCNs are the most prevelant and broadly applied model of GNNs, [1] which are able to leverage both the information of the nodes and their locality to make predictions [2]. They do this using a convolution layer, which merges the features of a node with those of its neighbours. The most common method of recommender systems used with GCNs is Collaborative Filtering, where user's past interactions and connections to other users is used to make predictions. The main way it does this is through learning latent features (or an embedding) for each user and item node. GCNs then update this embedding by propagating information across the graph:\n",
    "\n",
    "Let $\\mathbf{e}_u^{(0)}$ denote the user embedding of user $u$, and $\\mathbf{e}_i^{(0)}$ the item embedding of item $i$. Then after $k$ layers of propagation, we have:\n",
    "\n",
    "$$\\mathbf{e}_u^{(k+1)} = \\sigma \\left( \\mathbf{W}_1 \\mathbf{e}_u^{(k)} + \\sum_{i\\in \\mathcal{N}_u} \\frac{1}{\\sqrt{|\\mathcal{N}_u||\\mathcal{N}_i|}} (\\mathbf{W}_1\\mathbf{e}_u^{(k)} + \\mathbf{W}_2(\\mathbf{e}_i^{(k)}\\odot \\mathbf{e}_u^{(k)}))\\right)$$\n",
    "\n",
    "and a similar equation for $\\mathbf{e}_i^{(k+1)}$ [3]. In this equation, $\\sigma$ is the nonlinear activation function (e.g. ReLU), $\\mathcal{N}_u$ denotes the set of items that are interacted with by user $u$, and similar for $\\mathcal{N}_i$ and item $i$. $\\mathbf{W}_1$ and $\\mathbf{W}_2$ are trainable weight matrices used to perform feature transformations in each layer.\n",
    "\n",
    "These equations represent the basic building block of most GCNs, however after recent investigations as published in paper [3], it was discovered that for recommendation systems in particular, both the activation function, $\\sigma$, and the feature transformation matrices, $\\mathbf{W}_1, \\mathbf{W}_2$, have little influence on the model and in fact make the predictions worse in some situations. As such a new model was suggested that stripped these parts for a much simpler propogation method:\n",
    "\n",
    "$$\\mathbf{e}_u^{(k+1)} = \\sum_{i\\in \\mathcal{N}_u} \\frac{1}{\\sqrt{|\\mathcal{N}_u||\\mathcal{N}_i|}} \\mathbf{e}_i^{(k)}$$\n",
    "\n",
    "$$\\mathbf{e}_i^{(k+1)} = \\sum_{u\\in \\mathcal{N}_i} \\frac{1}{\\sqrt{|\\mathcal{N}_i||\\mathcal{N}_u|}} \\mathbf{e}_u^{(k)}$$\n",
    "\n",
    "This model is known as LightGCN, and is the model that will be explored in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Papers With Code: Graph Models - https://paperswithcode.com/methods/category/graph-models\n",
    "\n",
    "[2] Towards Data Science: Graph Convolutional Networks - https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95\n",
    "\n",
    "[3] Xiangnan He, et al. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation - https://arxiv.org/abs/2002.02126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
