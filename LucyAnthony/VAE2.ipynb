{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is exactly the same as VAE1.ipynb, except that we have scaled the matrices before inputting them into the VAE. Also we have printed the decoded matrix in order to explicitly compare artist preferences with predicted preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv('../unpushed_work/last_fm_data/user_artists.dat', sep='\\t')\n",
    "\n",
    "user_interaction_counts = user_data.groupby('userID').size()\n",
    "users_with_50_interactions = user_interaction_counts[user_interaction_counts >= 50].index\n",
    "user_data_filtered = user_data[user_data['userID'].isin(users_with_50_interactions)]\n",
    "\n",
    "unique_users = user_data_filtered['userID'].unique()\n",
    "\n",
    "# Ensure that test users have at least 50 interactions in 'test_data'\n",
    "# We need to carefully select 'test_users' to satisfy this condition\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "shuffled_users = np.random.permutation(unique_users)\n",
    "\n",
    "train_users = []\n",
    "test_users = []\n",
    "\n",
    "# We'll collect test users until we have enough that have at least 50 interactions\n",
    "for user in shuffled_users:\n",
    "    user_data_temp = user_data_filtered[user_data_filtered['userID'] == user]\n",
    "    if len(test_users) < int(0.2 * len(unique_users)):\n",
    "        # Tentatively add to test_users\n",
    "        test_users.append(user)\n",
    "    else:\n",
    "        train_users.append(user)\n",
    "\n",
    "# Recreate test_data and train_data\n",
    "train_data = user_data_filtered[user_data_filtered['userID'].isin(train_users)]\n",
    "test_data = user_data_filtered[user_data_filtered['userID'].isin(test_users)]\n",
    "\n",
    "# Now check that each user in test_data has 50 interactions\n",
    "# Remove any users from test_users who don't meet this criterion\n",
    "valid_test_users = []\n",
    "for user in test_users:\n",
    "    user_data_temp = test_data[test_data['userID'] == user]\n",
    "    if len(user_data_temp) == 50:\n",
    "        valid_test_users.append(user)\n",
    "\n",
    "# Update test_users and test_data\n",
    "test_users = valid_test_users\n",
    "test_data = test_data[test_data['userID'].isin(test_users)]\n",
    "\n",
    "# Update train_data to include any users removed from test_users\n",
    "removed_test_users = set(shuffled_users) - set(train_users) - set(test_users)\n",
    "if removed_test_users:\n",
    "    train_users.extend(list(removed_test_users))\n",
    "    train_data = user_data_filtered[user_data_filtered['userID'].isin(train_users)]\n",
    "\n",
    "# Now proceed to split test_data into test_x and test_y\n",
    "test_x = pd.DataFrame(columns=test_data.columns)\n",
    "test_y = pd.DataFrame(columns=test_data.columns)\n",
    "\n",
    "for user in test_users:\n",
    "    user_data_temp = test_data[test_data['userID'] == user]\n",
    "    user_data_shuffled = user_data_temp.sample(frac=1, random_state=42)\n",
    "    user_test_x = user_data_shuffled.iloc[:25]\n",
    "    user_test_y = user_data_shuffled.iloc[25:50]\n",
    "    test_x = pd.concat([test_x, user_test_x], ignore_index=True)\n",
    "    test_y = pd.concat([test_y, user_test_y], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode artistIDs\n",
    "# Essential since ML models require numerical input (also efficient)\n",
    "artist_encoder = LabelEncoder()\n",
    "all_artistIDs = user_data_filtered['artistID'].unique()\n",
    "artist_encoder.fit(all_artistIDs)\n",
    "num_artists = len(artist_encoder.classes_)\n",
    "\n",
    "# Encode train users\n",
    "train_user_encoder = LabelEncoder()\n",
    "train_user_encoder.fit(train_users)\n",
    "num_train_users = len(train_user_encoder.classes_)\n",
    "\n",
    "# Encode test users\n",
    "test_user_encoder = LabelEncoder()\n",
    "test_user_encoder.fit(test_users)\n",
    "num_test_users = len(test_user_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create user-item interaction matrix\n",
    "def create_user_item_matrix(data, user_encoder, num_users):\n",
    "    user_item_matrix = np.zeros((num_users, num_artists))\n",
    "    for idx, row in data.iterrows():\n",
    "        user_idx = user_encoder.transform([row['userID']])[0]\n",
    "        artist_idx = artist_encoder.transform([row['artistID']])[0]\n",
    "        weight = row['weight']\n",
    "        user_item_matrix[user_idx, artist_idx] = weight\n",
    "    return user_item_matrix\n",
    "\n",
    "train_user_item_matrix = create_user_item_matrix(train_data, train_user_encoder, num_train_users)\n",
    "test_x_user_item_matrix = create_user_item_matrix(test_x, test_user_encoder, num_test_users)\n",
    "test_y_user_item_matrix = create_user_item_matrix(test_y, test_user_encoder, num_test_users)\n",
    "\n",
    "# Scale data before inputting it into autoencoder\n",
    "# Find the maximum and minimum values across all matrices for consistent scaling\n",
    "max_value = max(train_user_item_matrix.max(), test_x_user_item_matrix.max(), test_y_user_item_matrix.max())\n",
    "min_value = min(train_user_item_matrix.min(), test_x_user_item_matrix.min(), test_y_user_item_matrix.min())\n",
    "\n",
    "# Define a function to normalize a matrix\n",
    "def normalise(matrix, min_value, max_value):\n",
    "    return (matrix - min_value) / (max_value - min_value)\n",
    "\n",
    "# Normalize each matrix\n",
    "train_user_item_matrix = normalise(train_user_item_matrix, min_value, max_value)\n",
    "test_x_user_item_matrix = normalise(test_x_user_item_matrix, min_value, max_value)\n",
    "test_y_user_item_matrix = normalise(test_y_user_item_matrix, min_value, max_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterise(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std) # Adds noise by sampling from standard normal dist.\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.fc_decode(z))\n",
    "        return torch.sigmoid(self.fc_out(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2214.3641\n",
      "Epoch 2, Loss: 221.0252\n",
      "Epoch 3, Loss: 19.4951\n",
      "Epoch 4, Loss: 10.9065\n",
      "Epoch 5, Loss: 10.1564\n",
      "Epoch 6, Loss: 9.6393\n",
      "Epoch 7, Loss: 8.9696\n",
      "Epoch 8, Loss: 8.2749\n",
      "Epoch 9, Loss: 8.1322\n",
      "Epoch 10, Loss: 7.6201\n"
     ]
    }
   ],
   "source": [
    "# Convert train data to tensor\n",
    "train_tensor = torch.FloatTensor(train_user_item_matrix)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "train_dataset = torch.utils.data.TensorDataset(train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "input_dim = num_artists\n",
    "hidden_dim = 256\n",
    "latent_dim = 50\n",
    "\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data_batch in train_loader:\n",
    "        data = data_batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {train_loss / len(train_loader.dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.0002\n"
     ]
    }
   ],
   "source": [
    "# Convert test data to tensor\n",
    "test_x_tensor = torch.FloatTensor(test_x_user_item_matrix)\n",
    "test_y_tensor = torch.FloatTensor(test_y_user_item_matrix)\n",
    "\n",
    "# Get the model's predictions on test_x\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recon_test_x, _, _ = model(test_x_tensor) # Reconstructed predictions\n",
    "\n",
    "# Compute MSE only on positions where test_y has interactions\n",
    "test_y_mask = test_y_tensor > 0\n",
    "predicted_ratings = recon_test_x[test_y_mask]\n",
    "true_ratings = test_y_tensor[test_y_mask]\n",
    "\n",
    "mse_loss = nn.functional.mse_loss(predicted_ratings, true_ratings)\n",
    "print(f\"Test MSE: {mse_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 True Artists for User 123 : [  88 1390 2610  768 2608   56 2824 3028 1546  366]\n",
      "Top 10 Predicted Artists for User 123 : [ 3695 15400  9928  2782  2182  5286  4923 11906  8404 17039]\n"
     ]
    }
   ],
   "source": [
    "def get_top_10_artist_ids(userID, recon_test_x, test_y_tensor, test_user_encoder, artist_encoder):\n",
    "    # Get the index of the user in the tensor\n",
    "    if userID not in test_user_encoder.classes_:\n",
    "        print(\"User ID not found in the test set.\")\n",
    "        return\n",
    "    \n",
    "    # Converts userID to its index\n",
    "    user_idx = test_user_encoder.transform([userID])[0]\n",
    "    \n",
    "    # Get true ratings and predicted ratings for this user\n",
    "    true_ratings = test_y_tensor[user_idx]\n",
    "    predicted_ratings = recon_test_x[user_idx]\n",
    "    \n",
    "    # Get the indices of the top 10 true artists (by highest ratings)\n",
    "    top_true_artist_indices = torch.topk(true_ratings, k=10).indices\n",
    "    # Convert the indices back to artistIDs\n",
    "    top_true_artists = artist_encoder.inverse_transform(top_true_artist_indices.cpu().numpy())\n",
    "    \n",
    "    # Get the indices of the top 10 predicted artists (by highest predicted ratings)\n",
    "    top_predicted_artist_indices = torch.topk(predicted_ratings, k=10).indices\n",
    "    top_predicted_artists = artist_encoder.inverse_transform(top_predicted_artist_indices.cpu().numpy())\n",
    "    \n",
    "    # Output the top 10 true and predicted artists\n",
    "    print(\"Top 10 True Artists for User\", userID, \":\", top_true_artists)\n",
    "    print(\"Top 10 Predicted Artists for User\", userID, \":\", top_predicted_artists)\n",
    "\n",
    "# Example usage:\n",
    "get_top_10_artist_ids(userID=123, recon_test_x=recon_test_x, test_y_tensor=test_y_tensor, \n",
    "                   test_user_encoder=test_user_encoder, artist_encoder=artist_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 True Artists for User 1104:\n",
      "1. Westlife\n",
      "2. Björk\n",
      "3. Prince\n",
      "4. Madonna\n",
      "5. Mariah Carey\n",
      "6. Michael Jackson\n",
      "7. Donna Summer\n",
      "8. Air Supply\n",
      "9. Meat Loaf\n",
      "10. Kylie Minogue\n",
      "\n",
      "Top 10 Recommended Artists for User 1104:\n",
      "1. Anthony Callea\n",
      "2. Ada Milea (cu Alexander Balanescu)\n",
      "3. Edda\n",
      "4. Burnt Fur\n",
      "5. Aubrey Ashburn\n",
      "6. NOMAK\n",
      "7. Otis Rush\n",
      "8. Placebo (Feat. David Bowie)\n",
      "9. Eberhard Weber\n",
      "10. WC\n"
     ]
    }
   ],
   "source": [
    "artist_ids = pd.read_csv('../unpushed_work/last_fm_data/artists.dat', sep='\\t')\n",
    "\n",
    "artist_id_to_name = pd.Series(artist_ids.name.values, index=artist_ids.id).to_dict()\n",
    "name_to_artist_id = {v: k for k, v in artist_id_to_name.items()}\n",
    "\n",
    "def get_top_10_artists(userID, recon_test_x, test_y_tensor, test_user_encoder, artist_encoder, artist_id_to_name):\n",
    "    # Get the index of the user in the tensor\n",
    "    if userID not in test_user_encoder.classes_:\n",
    "        print(\"User ID not found in the test set.\")\n",
    "        return\n",
    "    \n",
    "    user_idx = test_user_encoder.transform([userID])[0]\n",
    "    \n",
    "    # Get true ratings and predicted ratings for this user\n",
    "    true_ratings = test_y_tensor[user_idx]\n",
    "    predicted_ratings = recon_test_x[user_idx]\n",
    "    \n",
    "    # Get the indices of the top 10 true artists (by highest ratings)\n",
    "    top_true_artist_indices = torch.topk(true_ratings, k=10).indices\n",
    "    # Convert the indices back to artistIDs\n",
    "    top_true_artist_ids = artist_encoder.inverse_transform(top_true_artist_indices.cpu().numpy())\n",
    "    top_true_artist_names = [artist_id_to_name.get(artist_id, \"Unknown Artist\") for artist_id in top_true_artist_ids]\n",
    "    \n",
    "    # Get the indices of the top 10 predicted artists (by highest predicted ratings)\n",
    "    top_predicted_artist_indices = torch.topk(predicted_ratings, k=10).indices\n",
    "    top_predicted_artist_ids = artist_encoder.inverse_transform(top_predicted_artist_indices.cpu().numpy())\n",
    "    top_predicted_artist_names = [artist_id_to_name.get(artist_id, \"Unknown Artist\") for artist_id in top_predicted_artist_ids]\n",
    "    \n",
    "    # Output the top 10 true and predicted artist names\n",
    "    print(f\"Top 10 True Artists for User {userID}:\")\n",
    "    for idx, artist_name in enumerate(top_true_artist_names, start=1):\n",
    "        print(f\"{idx}. {artist_name}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 Recommended Artists for User {userID}:\")\n",
    "    for idx, artist_name in enumerate(top_predicted_artist_names, start=1):\n",
    "        print(f\"{idx}. {artist_name}\")\n",
    "\n",
    "get_top_10_artists(userID=1104, recon_test_x=recon_test_x, test_y_tensor=test_y_tensor, \n",
    "                   test_user_encoder=test_user_encoder, artist_encoder=artist_encoder, artist_id_to_name=artist_id_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_generator(artist_preferences):\n",
    "\n",
    "    columns = ['userID', 'artistID', 'weight']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    rows = []\n",
    "    weight = 10\n",
    "    for i in artist_preferences:\n",
    "        artistID = name_to_artist_id.get(i, None)\n",
    "        if artistID is None:\n",
    "            print('The artist ', i, ' unfortunately does not appear in our records, please pick a different artist')\n",
    "            return\n",
    "        if artistID is not None:\n",
    "            new_row = {'userID' : 2, 'artistID': artistID, 'weight' : weight}\n",
    "            weight -= 1\n",
    "            rows.append(new_row)\n",
    "    df = pd.concat([df, pd.DataFrame(rows)], ignore_index=True)\n",
    "    \n",
    "    # Encode users\n",
    "    artist_ids = pd.read_csv('../unpushed_work/last_fm_data/artists.dat', sep='\\t')\n",
    "    all_artist_ids = artist_ids.id.tolist()\n",
    "    df_user_encoder = LabelEncoder()\n",
    "    df_user_encoder.fit(all_artist_ids)\n",
    "    num_df_users = len(df_user_encoder.classes_)\n",
    "    df_matrix = create_user_item_matrix(df, df_user_encoder, num_df_users)\n",
    "\n",
    "    # Scale data before inputting it into autoencoder\n",
    "    max_value = df_matrix.max()\n",
    "    min_value = df_matrix.min()\n",
    "    # Define a function to normalize a matrix\n",
    "    def normalise(matrix, min_value, max_value):\n",
    "        return (matrix - min_value) / (max_value - min_value)\n",
    "\n",
    "    # Normalize each matrix\n",
    "    df_matrix = normalise(df_matrix, min_value, max_value)\n",
    "\n",
    "    # Convert test data to tensor\n",
    "    df_tensor = torch.FloatTensor(df_matrix)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction, _, _ = model(df_tensor) # Reconstructed predictions\n",
    "    \n",
    "    prediction = prediction[2]\n",
    "    \n",
    "    # Get the indices of the top 10 predicted artists (by highest predicted ratings)\n",
    "    top_predicted_artist_indices = torch.topk(prediction, k=10).indices\n",
    "    top_predicted_artist_ids = artist_encoder.inverse_transform(top_predicted_artist_indices.cpu().numpy())\n",
    "    top_predicted_artist_names = [artist_id_to_name.get(artist_id, \"Unknown Artist\") for artist_id in top_predicted_artist_ids]\n",
    "\n",
    "    print(top_predicted_artist_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Burning the Masses', 'Barrelhouse', 'Bora Uzer', 'Tyler Hilton', 'Dancing Ghosts', 'Noon', 'Tunng', 'Brandy', 'Kathryn Williams', 'Freak Kitchen']\n"
     ]
    }
   ],
   "source": [
    "lucy_preferences = ['Amy Winehouse', 'The Strokes', 'Radiohead', 'Pink Floyd', 'Red Hot Chili Peppers', 'Bob Marley & The Wailers', 'The Beatles', 'Jimi Hendrix', 'Muse', 'Tame Impala'] # We assume this is list of artist names of length 10 ordered from highest to lowest preference\n",
    "\n",
    "recommendation_generator(lucy_preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
