{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Collaborative Filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Theory\n",
    "\n",
    "Collaborative filtering creates a user-item matrix with values corresponding to the users preferences. Next, using a chosen similarity metric, the similarities between users' preferences are used to give recommendations for each user. Each user will be given recommendations for items that they have not given feedback for, but have positive feedback from users similar to the chosen user. These recommendations may also be predictions.\n",
    "\n",
    "## 0.1: Similarity\n",
    "There are different similarity measures that can be chosen. The Pearson correlation coefficient measures linear relation between two variables, and the cosine similarity measures the simialrirty between two vectors depending on the angle between them in a vector space. Similarity may also be referenced as the distance metric or correlation metric.\n",
    "\n",
    "The two types of collaborative filtering techniques are model-based and memory-based.\n",
    "\n",
    "## 0.2: Memory-based Methods\n",
    "Memory-based collaborative filtering can be user-based or item-based. User-based techniques compute the similarities between users based on their implicit feedback for the same item. Then, the predicted rating or given feedback is calculated using weighted averages of the item's ratings given by similar users. The weights are the similarities of the other users with the chosen item. Item-based techniques work similalrly but use the similarity between items instead of the similarity between users. Both of these methods form a similarity matrix.\n",
    "\n",
    "## 0.3: Model-based Methods\n",
    "Model-based collaborative filtering can be a lot quicker than memory-based methods. An example of this is the singular value decomposition (SVD). These methods use the user-item matrix to find rules between items and uses these rules to give a list of recommendations. If data is sparse, then model-based methods are recommended to deal with this. More advanced model-based recommendation systems can use clustering, neural networks and elements of graph theory. The main drawback of model-based methods is that they are typically have a very high computational cost and may require a large amount of memory.\n",
    "\n",
    "The most popular algorithm used for collaborative filtering, when the similarity matrix is sparse, is Alternating Least Squares (ALS) minimisation. Simply, this aims to estimate the entries of a matrix $M=UV^T$ when only a subset of these entries is observed. The algorithm minimises the squared error with the observed entries, when alternating in optimising $U$ and $V$. This would allow us to give predicted entries for items which a given user has not listened to yet.\n",
    "\n",
    "## 0.4: Pros & Cons\n",
    "Collaborative filtering can be used when data is difficult to analyse since it can use the imnplicit feedback. However, there are a few problems. Firstly, the cold-start problem - a new user has no data, hence, the system cannot make meaningful recommendations for them. Also, if data is sparse, then recommendations can be less accurate and many items may not be recommended at all. Finally, the method must be scalable in order to stay efficient. The basic collaborative filtering methods can struggle with this, but model-based methods like SVD can be used to give efficient and robust recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Data Preparation\n",
    "## 1.1 Loading Data & Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements03.txt\n",
    "# This can be used to install the necessary modules if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "import implicit\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "artists = pd.read_csv(os.path.join('..','data','artists.dat'), delimiter='\\t')\n",
    "tags = pd.read_csv(os.path.join('..','data','tags.dat'), delimiter='\\t',encoding='ISO-8859-1')\n",
    "user_artists = pd.read_csv(os.path.join('..','data','user_artists.dat'), delimiter='\\t')\n",
    "user_friends = pd.read_csv(os.path.join('..','data','user_friends.dat'), delimiter='\\t')\n",
    "user_taggedartists_timestamps = pd.read_csv(os.path.join('..','data','user_taggedartists-timestamps.dat'), delimiter='\\t')\n",
    "user_taggedartists = pd.read_csv(os.path.join('..','data','user_taggedartists.dat'), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns from the Artists dataset\n",
    "artists_cleaned = artists.drop(columns=['url', 'pictureURL']).drop_duplicates(keep='first') \n",
    "\n",
    "# Drop the irrelevant columns in the Tags dataset\n",
    "tags_cleaned = tags.drop_duplicates(keep='first') \n",
    "\n",
    "# For the User-Artists dataset, we can filter out rows with a weight of 0, as they show no meaningful interaction\n",
    "user_artists_cleaned = user_artists[user_artists['weight'] > 0]\n",
    "user_artists_cleaned = user_artists_cleaned.drop_duplicates(keep='first') \n",
    "\n",
    "# Drop duplicates from the User-Tagged Artists Timestamps dataset\n",
    "user_taggedartists_timestamps_cleaned = user_taggedartists_timestamps.drop_duplicates(keep='first') \n",
    "\n",
    "# Convert timestamps from ms to datetime format\n",
    "user_taggedartists_timestamps_cleaned['timestamp'] = pd.to_datetime(user_taggedartists_timestamps_cleaned['timestamp'], unit='ms')\n",
    "\n",
    "# Drop duplicates from the User-Friends dataset\n",
    "user_friends_cleaned = user_friends.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to output cleaned datasets for inspection\n",
    "# print(\"Cleaned Artists dataset:\", artists_cleaned.info(), artists_cleaned.head())\n",
    "# print(\"Cleaned Tags dataset:\", tags_cleaned.info(), tags_cleaned.head())\n",
    "print(\"Cleaned User-Artists dataset:\", user_artists_cleaned.info(), user_artists_cleaned.head())\n",
    "# print(\"Cleaned User-Tagged Artists Timestamps dataset:\", user_taggedartists_timestamps_cleaned.info(), user_taggedartists_timestamps_cleaned.head())\n",
    "# print(\"Cleaned User-Friends dataset:\", user_friends_cleaned.info(), user_friends_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Implementing Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement collaborative filtering using some of the different techniques that we have described. We will use some memory-based and some model-based methods.\n",
    "We will investigate how these methods give recommendations and how efficient they are at doing so.\n",
    "\n",
    "First, we create a dictionary which will allow us to map artistID recommendations to the corresponding names of the artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map artistID to artistName\n",
    "artist_id_to_name = dict(zip(artists['id'], artists['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise the user-artist matrix which has values corresponding to the listening counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-artist interaction matrix using the user_artists_cleaned dataset\n",
    "user_artist_matrix = user_artists_cleaned.pivot(index='userID', columns='artistID', values='weight')\n",
    "\n",
    "# Fill NaN values with 0s (assuming binary or implicit feedback, i.e., 1 for interaction, 0 for no interaction)\n",
    "user_artist_matrix = user_artist_matrix.fillna(0)\n",
    "\n",
    "print(user_artist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test split\n",
    "train_data, test_data = train_test_split(user_artists_cleaned, test_size=0.2, random_state=27)\n",
    "\n",
    "# Initialize train and test matrices directly with 0s\n",
    "train_matrix = pd.DataFrame(0, index=user_artist_matrix.index, columns=user_artist_matrix.columns)\n",
    "test_matrix = pd.DataFrame(0, index=user_artist_matrix.index, columns=user_artist_matrix.columns)\n",
    "\n",
    "# Populate train and test matrices\n",
    "for _, row in train_data.iterrows():\n",
    "    train_matrix.at[row['userID'], row['artistID']] = row['weight']\n",
    "\n",
    "for _, row in test_data.iterrows():\n",
    "    test_matrix.at[row['userID'], row['artistID']] = row['weight']\n",
    "\n",
    "# Compute cosine similarity\n",
    "user_similarity = cosine_similarity(train_matrix)\n",
    "user_similarity_df = pd.DataFrame(user_similarity, index=train_matrix.index, columns=train_matrix.index)\n",
    "\n",
    "# Dictionary to map artistID to artistName\n",
    "artist_id_to_name = dict(zip(artists['id'], artists['name']))\n",
    "\n",
    "# User-based recommendation function\n",
    "def get_user_based_recommendations(user_id, user_similarity_df, train_matrix, artist_id_to_name, top_n=10):\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False).index[1:]\n",
    "    recommendations = {}\n",
    "\n",
    "    for similar_user in similar_users:\n",
    "        interacted_artists = train_matrix.loc[similar_user][train_matrix.loc[similar_user] > 0].index.tolist()\n",
    "        for artist in interacted_artists:\n",
    "            if artist not in train_matrix.loc[user_id][train_matrix.loc[user_id] > 0].index.tolist():\n",
    "                recommendations[artist] = recommendations.get(artist, 0) + user_similarity_df[user_id][similar_user]\n",
    "\n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [(artist, artist_id_to_name.get(artist, \"Unknown\"), score) for artist, score in sorted_recommendations[:top_n]]\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get recommendations for user 2\n",
    "user_id = 2\n",
    "user_based_recommendations = get_user_based_recommendations(user_id, user_similarity_df, train_matrix, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display recommendations\n",
    "print(\"Top User-Based Recommendations for User 2:\")\n",
    "for artist_id, artist_name, score in user_based_recommendations:\n",
    "    print(f\"Artist ID: {artist_id}, Artist: {artist_name}, Similarity Score: {score:.2f}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and display elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time elapsed: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Memory-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 User-Based Implementation\n",
    "For the user-based implementation, we must compute the similarity matrix using the cosine similarity between users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cosine similarity between users\n",
    "user_similarity = cosine_similarity(user_artist_matrix)\n",
    "\n",
    "# Convert the similarity matrix into a DataFrame for easy inspection\n",
    "user_similarity_df = pd.DataFrame(user_similarity, index=user_artist_matrix.index, columns=user_artist_matrix.index)\n",
    "\n",
    "# Display a portion of the user similarity matrix\n",
    "print(user_similarity_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function which gives user-based recommendations. This function uses the user-artist matrix and the similarity matrix to give the top-$N$ recommendations for the given user. It then maps the corresponding ID recommendations of artists to the artist names using the previously defined dictionary.\n",
    "\n",
    "The implementation is shown for user 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get user-based recommendations\n",
    "def get_user_based_recommendations(user_id, user_similarity_df, user_artist_matrix, artist_id_to_name, top_n=10):    \n",
    "    # Get the most similar users (excluding the user itself)\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False).index[1:]\n",
    "\n",
    "    recommendations = {}\n",
    "    for similar_user in similar_users:\n",
    "        # Get the artists this similar user has interacted with (non-zero values)\n",
    "        interacted_artists = user_artist_matrix.loc[similar_user][user_artist_matrix.loc[similar_user] > 0].index.tolist()\n",
    "\n",
    "        for artist in interacted_artists:\n",
    "            # Only consider artists the target user has not interacted with\n",
    "            if artist not in user_artist_matrix.loc[user_id][user_artist_matrix.loc[user_id] > 0].index.tolist():\n",
    "                # Add the artist to recommendations with a score (using the scaled similarity as a weight)\n",
    "                if artist not in recommendations:\n",
    "                    recommendations[artist] = user_similarity_df[user_id][similar_user]\n",
    "                else:\n",
    "                    # Add the weight of similarity to the current score\n",
    "                    recommendations[artist] += user_similarity_df[user_id][similar_user]\n",
    "\n",
    "    # Sort recommendations by score (highest first)\n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Convert artist IDs to names and prepare the final list with IDs, names, and scores\n",
    "    recommended_artists = [(artist, artist_id_to_name.get(artist, \"Unknown\"), score) for artist, score in sorted_recommendations[:top_n]]\n",
    "\n",
    "    return recommended_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 user-based recommendations for user with userID=2\n",
    "user_id = 2\n",
    "user_based_recommendations = get_user_based_recommendations(user_id, user_similarity_df, user_artist_matrix, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display user-based recommendations\n",
    "print(\"Top User-Based Recommendations for User 2:\")\n",
    "for artist_id, artist_name, score in user_based_recommendations:\n",
    "    print(f\"Artist ID: {artist_id}, Artist: {artist_name}, Similarity Score: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# calculate and display elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time elapsed: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the output is the top 5 recommendations for user 2 with their similarity scores. The output has recommended 5 female pop singers which is interesting. We will now give recommendations for user 3. We also take note of the time taken to give the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 user-based recommendations for user with userID=3\n",
    "user_id = 3\n",
    "user_based_recommendations = get_user_based_recommendations(user_id, user_similarity_df, user_artist_matrix, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display user-based recommendations\n",
    "print(\"Top User-Based Recommendations for User 3:\")\n",
    "for artist_id, artist_name, score in user_based_recommendations:\n",
    "    print(f\"Artist ID: {artist_id}, Artist: {artist_name}, Similarity Score: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# calculate and display elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time elapsed: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artists recommended for user 3 have much lower similarity scores than those recommended to user 2. In this case, the top 4 recommendations all could be categorised as electronic artists. However, the fifth recommendation 'Radiohead' is not very similar to the others.  We also take note of the time taken to give the recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Item-based Implementation\n",
    "For the item-based implementation, we must compute the similarity matrix using the cosine similarity between artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cosine similarity between artists (transpose the matrix to compare artists)\n",
    "artist_similarity = cosine_similarity(user_artist_matrix.T)  # Transpose to compare artists (columns)\n",
    "\n",
    "# Convert the similarity matrix into a DataFrame for easy inspection\n",
    "artist_similarity_df = pd.DataFrame(artist_similarity, index=user_artist_matrix.columns, columns=user_artist_matrix.columns)\n",
    "\n",
    "# Display a portion of the artist similarity matrix\n",
    "print(artist_similarity_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function which gives item-based recommendations. This function uses the user-artist matrix and the similarity matrix to give the top-$N$ recommendations for the given user. It then maps the corresponding ID recommendations of artists to the artist names using the previously defined dictionary.\n",
    "\n",
    "The implementation is shown for user 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get item-based recommendations\n",
    "def get_item_based_recommendations(user_id, user_artist_matrix, artist_similarity_df, artist_id_to_name, top_n=10):\n",
    "    # Get the artists the user has interacted with (non-zero values)\n",
    "    interacted_artists = user_artist_matrix.loc[user_id][user_artist_matrix.loc[user_id] > 0].index.tolist()\n",
    "    \n",
    "    recommendations = {}\n",
    "    for artist in interacted_artists:\n",
    "        # Get the most similar artists to the ones the user interacted with\n",
    "        similar_artists = artist_similarity_df[artist].sort_values(ascending=False).index[1:]  # Exclude the artist itself\n",
    "\n",
    "        for similar_artist in similar_artists:\n",
    "            # Add the similar artist to recommendations with a score (using the similarity as a weight)\n",
    "            if similar_artist not in recommendations:\n",
    "                recommendations[similar_artist] = artist_similarity_df[artist][similar_artist]\n",
    "            else:\n",
    "                recommendations[similar_artist] += artist_similarity_df[artist][similar_artist]\n",
    "\n",
    "    # Sort recommendations by score (highest first)\n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Convert artist IDs to names using artist_id_to_name\n",
    "    recommended_artists = [(artist_id, artist_id_to_name.get(artist_id, \"Unknown\"), score) for artist_id, score in sorted_recommendations[:top_n]]\n",
    "\n",
    "    return recommended_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 item-based recommendations for user with userID=2\n",
    "user_id = 2\n",
    "item_based_recommendations = get_item_based_recommendations(user_id, user_artist_matrix, artist_similarity_df, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display item-based recommendations\n",
    "print(\"\\nTop Item-Based Recommendations for User 2:\")\n",
    "for artist_id, artist_name, score in item_based_recommendations:\n",
    "    print(f\"Artist ID: {artist_id}, Artist: {artist_name}, Similarity Score: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the item-based method gives different recommendations to the user-based method. The similarity scores for the top 5 recommendations are higher for the item-based model. We can see that the item-based method is much faster than the user-based method for this datset. This is because the user-based function iterates over both similar users and artists, whereas as the item-based method only iterates over similar artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 item-based recommendations for user with userID=3\n",
    "user_id = 3\n",
    "item_based_recommendations = get_item_based_recommendations(user_id, user_artist_matrix, artist_similarity_df, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display item-based recommendations\n",
    "print(\"\\nTop Item-Based Recommendations for User 3:\")\n",
    "for artist_id, artist_name, score in item_based_recommendations:\n",
    "    print(f\"Artist ID: {artist_id}, Artist: {artist_name}, Similarity Score: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see that the item-based method gives different recommendations to the user-based method. The similarity scores for the top 5 recommendations are much higher for the item-based model for user 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model-based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Singular Value Decomposition\n",
    "We compute the SVD of the user-artist matrix using `scikit-learn` and use this to give recommendations. The SVD may help to identify patterns in the data and knowledge of these could improve our receommendations. We apply a SVD model to the user-artist matrix to get the SVD components (artist features), and then approximate the original user-artist matrix. This approximation matrix is what we use to make our recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svd_recommendations(user_id, user_artist_matrix, artist_id_to_name, top_n=10, n_components=50):\n",
    "    # Apply SVD to the user-artist matrix\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    svd_matrix = svd.fit_transform(user_artist_matrix)\n",
    "    svd_components = svd.components_\n",
    "\n",
    "    # Reconstruct the user-artist interaction matrix\n",
    "    reconstructed_matrix = np.dot(svd_matrix, svd_components)\n",
    "    \n",
    "    recommendations = {}\n",
    "        \n",
    "    # Get the user's interaction vector from the reconstructed matrix\n",
    "    reconstructed_user_vector = reconstructed_matrix[user_id - 2]  # User IDs start at 2, so subtract 2\n",
    "    \n",
    "    # Iterate through all artists to recommend\n",
    "    for i, score in enumerate(reconstructed_user_vector):\n",
    "        # Check if the artist has been interacted with (score > 0) and if the artist ID is valid\n",
    "        if user_artist_matrix.iloc[user_id - 2, i] == 0:  # Ensure we only recommend non-interacted artists\n",
    "            artist_id = i  # The index of the artist in the matrix\n",
    "            if artist_id not in recommendations:\n",
    "                recommendations[artist_id] = score\n",
    "            else:\n",
    "                recommendations[artist_id] += score\n",
    "    \n",
    "    # Sort recommendations by score (highest first)\n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Convert artist IDs to names using the artist_id_to_name mapping\n",
    "    recommended_artists = [(artist_id, artist_id_to_name.get(artist_id, \"Unknown\"), score)\n",
    "                           for artist_id, score in sorted_recommendations[:top_n]]\n",
    "    \n",
    "    return recommended_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 SVD-based recommendations for user with userID=2\n",
    "user_id = 2\n",
    "svd_recommendations = get_svd_recommendations(user_id, user_artist_matrix, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display SVD-based recommendations\n",
    "print(\"\\nTop SVD-Based Recommendations for User 2:\")\n",
    "for artist_id, artist_name, score in svd_recommendations:\n",
    "    print(f\"Artist ID: {artist_id}, Artist: {artist_name}, Similarity Score: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 SVD-based recommendations for user with userID=3\n",
    "user_id = 3\n",
    "svd_recommendations = get_svd_recommendations(user_id, user_artist_matrix, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display SVD-based recommendations\n",
    "print(\"\\nTop SVD-Based Recommendations for User 3:\")\n",
    "for artist_id, artist_name, score in svd_recommendations:\n",
    "    print(f\"Artist ID: {artist_id}, Artist: {artist_name}, Similarity Score: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommendations for user 2 had significantly higher similarity scores compared to the user-based and item-based methods from before. However, for user 3, the SVD gave higher similarities than the user-based method but lower than the item-based method. This suggests that the performance of the method is highly dependenet on the available data and that this must be considered when choosing which technique we use. So far, the SVD method is the fastest at giving recommendations, since the SVD reduces the dimensionality of the user-artist matrix by keeping only the most important features, making computations more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 ALS Using `implicit`\n",
    "A library developed for efficient recommendation systems using Python is `implicit`. We can use this with a sparse matrix of user or item weights to give recommendations. We initialise using implicit.als.AlternatingLeastSquares() and then use .fit() and .recommend() to fit our model and give recommendations.\n",
    "\n",
    "We now implement the very popular ALS method using the `implicit` library for efficiency. We use the csr_matrix() function from `scipy` to convert the user-artist matrix to a sparse format that is suitbale fort ALS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_als_recommendations(user_id, user_artist_matrix, artist_id_to_name, top_n=5, factors=50, regularization=0.1, iterations=20):\n",
    "    # Convert the user-artist matrix to sparse format (csr_matrix)\n",
    "    sparse_matrix = csr_matrix(user_artist_matrix.values)\n",
    "    \n",
    "    # Initialize and train the ALS model\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=factors, regularization=regularization, iterations=iterations)\n",
    "    model.fit(sparse_matrix)\n",
    "\n",
    "    # Get the user's interaction vector (row from sparse matrix)\n",
    "    user_vector = sparse_matrix[user_id]\n",
    "\n",
    "    # Get top N artist recommendations (returns artist IDs and scores)\n",
    "    recommendations = model.recommend(user_id, user_vector, N=top_n)\n",
    "\n",
    "    # Convert artist IDs to artist names using the provided dictionary\n",
    "    recommended_artists = [(artist_id_to_name[artist_id], score) for artist_id, score in zip(recommendations[0], recommendations[1])]\n",
    "\n",
    "    return recommended_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 ALS-based recommendations for user with userID=2\n",
    "user_id = 2\n",
    "als_recommendations = get_als_recommendations(user_id, user_artist_matrix, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display ALS-based recommendations\n",
    "print(f\"\\nTop ALS-Based Recommendations for User {user_id}:\")\n",
    "for artist_name, score in als_recommendations:\n",
    "    print(f\"Artist: {artist_name}, Predicted Listening Count: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Example: Get top 5 ALS-based recommendations for user with userID=3\n",
    "user_id = 3\n",
    "als_recommendations = get_als_recommendations(user_id, user_artist_matrix, artist_id_to_name, top_n=5)\n",
    "\n",
    "# Display ALS-based recommendations\n",
    "print(f\"\\nTop ALS-Based Recommendations for User {user_id}:\")\n",
    "for artist_name, score in als_recommendations:\n",
    "    print(f\"Artist: {artist_name}, Predicted Listening Count: {score:.2f}\")\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this method returns predicted listening counts, unlike our other methods. All of the predicted listening counts for the top 5 recommendations are similar and are in the range 1.22-1.25. The artists are quite different in terms of genre. This method takes a similar amount of time as the SVD method for this dataset. This suggests that ALS is amn efficient method for giving recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Collaborative Filtering with PySpark\n",
    "Apache Spark is an engine used to process data at a large scale efficiently. It has APIs in Python and R. PySpark is the Python API for Apache Spark. PySpark has features which include Spark SQL, dataframes and machine learning.\n",
    "\n",
    "Using PySpark dataframes allows us to efficiently analyse and tranform data by using Python and Spark SQL together. Spark SQL is the Apache Spark module for using structured data, like dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 ALS Collaborative Filtering with PySpark\n",
    "We can use the `pyspark.ml` library to implement ALS. We import the ALS model from `pyspark.ml.recommendation` to create our model, then use .fit() and .recommendForAllUsers() and .recommendForAllItems() to make recommendations.\n",
    "\n",
    "We now implement ALS using PySpark methods to improve the efficiency of our recommender system, since it is highly scalable if we were to use our recommender system for very large datasets. We will try user-based and item-based implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"CollaborativeFilteringALS\").getOrCreate()\n",
    "\n",
    "# Convert cleaned pandas DataFrames to PySpark DataFrames\n",
    "artists_spark_df = spark.createDataFrame(artists_cleaned)\n",
    "user_artists_spark_df = spark.createDataFrame(user_artists_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 User-based PySpark ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# ALS model setup for user-based collaborative filtering\n",
    "# listening counts are implicit feedback, we are not starting from a cold-start\n",
    "als = ALS(userCol=\"userID\", itemCol=\"artistID\", ratingCol=\"weight\", coldStartStrategy=\"drop\", implicitPrefs=True)\n",
    "\n",
    "# Fit the ALS model\n",
    "model = als.fit(user_artists_spark_df)\n",
    "\n",
    "# Generate recommendations\n",
    "user_recommendations = model.recommendForAllUsers(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map artistID to artistName\n",
    "artist_id_to_name = {row['id']: row['name'] for row in artists_spark_df.collect()}\n",
    "\n",
    "# Function to map artistID to artistName and round scores to 2 decimal places\n",
    "def map_recommendations(user_recommendations):\n",
    "    def map_row(row):\n",
    "        recommendations_with_names = [\n",
    "            (artist_id_to_name.get(rec[0], \"Unknown\"), round(rec[1], 2)) for rec in row['recommendations']\n",
    "        ]\n",
    "        return (row['userID'], recommendations_with_names)\n",
    "\n",
    "    mapped_recommendations = user_recommendations.rdd.map(map_row).toDF([\"userID\", \"recommendations\"])\n",
    "    return mapped_recommendations\n",
    "\n",
    "# Apply the artistID to name mapping function\n",
    "user_recommendations_with_names = map_recommendations(user_recommendations)\n",
    "\n",
    "# Show the final recommendations with artist names and rounded scores\n",
    "user_recommendations_with_names.show(truncate=False)\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the output is recommendations for all users and this was computed very quickly, so the use of PySpark is effective. Note that the given time is much longer than the previous methods, but this method has givemn recommendations for all users, not just one user at a time. This clearly shows the efficiency and scalability of PySpark ALS for recommender systems.\n",
    "\n",
    "Also, the output gives recommendations with a 'score'. This is the relative confidence that a user will like a given artist. Clearly, some users, like user 16, have higher score but some users, like user 28, have much lower scores. This would indicate that the recommendations for user 16 relative to user 28 are much better. This is likely dependent on the availability of data for the different users.\n",
    "\n",
    "We will analyse the distribution of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the recommendations column into individual rows\n",
    "exploded_user_recommendations = user_recommendations.withColumn(\"recommendation\", F.explode(\"recommendations\"))\n",
    "\n",
    "# Extract artistID and rating\n",
    "exploded_user_recommendations = exploded_user_recommendations.select(\n",
    "    F.col(\"userID\"),\n",
    "    F.col(\"recommendation.artistID\").alias(\"artistID\"),  # Extract artistID\n",
    "    F.col(\"recommendation.rating\").alias(\"score\")        # Extract rating as score\n",
    ")\n",
    "\n",
    "# Generate summary statistics for the scores\n",
    "summary_stats = exploded_user_recommendations.select(\"score\").summary()\n",
    "\n",
    "# Display the summary statistics\n",
    "summary_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the scores range from around 0 to 2, with most of the recommendations being between 1.03 and 1.25. We then will say that the best recommendations are given by scores above 1.25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Item-based PySpark ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# ALS model setup for item-based collaborative filtering\n",
    "# Swap userCol and itemCol for item-based filtering\n",
    "# listening counts are implicit feedback, we are not starting from a cold-start\n",
    "als = ALS(userCol=\"artistID\", itemCol=\"userID\", ratingCol=\"weight\", coldStartStrategy=\"drop\", implicitPrefs=True)\n",
    "\n",
    "# Fit the ALS model\n",
    "model = als.fit(user_artists_spark_df)\n",
    "\n",
    "# Generate item-based recommendations for each artist (item)\n",
    "item_recommendations = model.recommendForAllItems(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the artistID to name mapping function\n",
    "item_recommendations_with_names = map_recommendations(item_recommendations)\n",
    "\n",
    "# Show the final recommendations with artist names and rounded scores\n",
    "item_recommendations_with_names.show(truncate=False)\n",
    "\n",
    "# end timing\n",
    "end_time = time.time()\n",
    "\n",
    "# print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the item-based implementation seems to be more efficient than the user-based due to the different iterations needed by each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the recommendations column into individual rows\n",
    "exploded_item_recommendations = item_recommendations.withColumn(\"recommendation\", F.explode(\"recommendations\"))\n",
    "\n",
    "# Extract artistID and rating from the struct fields\n",
    "exploded_item_recommendations = exploded_item_recommendations.select(\n",
    "    F.col(\"userID\"),\n",
    "    F.col(\"recommendation.artistID\").alias(\"artistID\"),  # Extract artistID\n",
    "    F.col(\"recommendation.rating\").alias(\"score\")        # Extract rating as score\n",
    ")\n",
    "\n",
    "# Generate summary statistics for the scores\n",
    "summary_stats = exploded_item_recommendations.select(\"score\").summary()\n",
    "\n",
    "# Display the summary statistics\n",
    "summary_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores for the item-based implementation have a similar distribution to the user-based implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Evaluating ALS Collaborative Filtering in PySpark\n",
    "Now that we have implemented memory-based and model-based methods with both Python and PySpark, we can clearly see that the PySpark ALS method is by far the most efficient and can give recommendations for all users quickly. From the collaborative filtering methods we have seen, this is the most suitable for our recommender system in practice, since scalability and efficiency is very important.\n",
    "\n",
    "We will now split the data into training and test sets to evaluaet the performance of this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# ALS model setup for user-based collaborative filtering\n",
    "als = ALS(userCol=\"userID\", itemCol=\"artistID\", ratingCol=\"weight\", coldStartStrategy=\"drop\", implicitPrefs=True, regParam=1.0)\n",
    "\n",
    "# Split data into training and test sets (80% training, 20% testing)\n",
    "train_data, test_data = user_artists_spark_df.randomSplit([0.8, 0.2], seed=27)\n",
    "\n",
    "# Fit the ALS model\n",
    "model = als.fit(train_data)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = predictions.withColumn(\"squared_error\", (F.col(\"prediction\") - F.col(\"weight\"))**2)\n",
    "rmse_value = rmse.select(F.sqrt(F.avg(\"squared_error\"))).first()[0]\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_value:.4f}\")\n",
    "\n",
    "# Generate recommendations\n",
    "user_recommendations = model.recommendForAllUsers(5)\n",
    "\n",
    "# Create a dictionary to map artistID to artistName\n",
    "artist_id_to_name = {row['id']: row['name'] for row in artists_spark_df.collect()}\n",
    "\n",
    "# Function to map artistID to artistName and round scores to 2 decimal places\n",
    "def map_recommendations(user_recommendations):\n",
    "    def map_row(row):\n",
    "        recommendations_with_names = [\n",
    "            (artist_id_to_name.get(rec[0], \"Unknown\"), round(rec[1], 2)) for rec in row['recommendations']\n",
    "        ]\n",
    "        return (row['userID'], recommendations_with_names)\n",
    "\n",
    "    mapped_recommendations = user_recommendations.rdd.map(map_row).toDF([\"userID\", \"recommendations\"])\n",
    "    return mapped_recommendations\n",
    "\n",
    "# Apply the artistID to name mapping function\n",
    "user_recommendations_with_names = map_recommendations(user_recommendations)\n",
    "\n",
    "# Show the final recommendations with artist names and rounded scores\n",
    "user_recommendations_with_names.show(truncate=False)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"Time elapsed: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is very high, suggesting that the recommendations are not very meaningful. This is likely due to the sparsity of the dataset.\n",
    "\n",
    "We will see if using regularisation parameter `regParam` may reduce the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: With regParam=0.5\n",
    "als_0_5 = ALS(userCol=\"userID\", itemCol=\"artistID\", ratingCol=\"weight\", coldStartStrategy=\"drop\", implicitPrefs=True, regParam=0.5)\n",
    "model_0_5 = als_0_5.fit(train_data)\n",
    "predictions_0_5 = model_0_5.transform(test_data)\n",
    "\n",
    "# Calculate RMSE for Experiment 1\n",
    "squared_error_0_5 = predictions_0_5.withColumn(\"squared_error\", (F.col(\"prediction\") - F.col(\"weight\")) ** 2)\n",
    "rmse_0_5_value = squared_error_0_5.select(F.sqrt(F.avg(\"squared_error\"))).first()[0]\n",
    "print(f\"RMSE with regParam=0.5: {rmse_0_5_value:.4f}\")\n",
    "\n",
    "# Experiment 2: With regParam=1.0\n",
    "als_1_0 = ALS(userCol=\"userID\", itemCol=\"artistID\", ratingCol=\"weight\", coldStartStrategy=\"drop\", implicitPrefs=True, regParam=1.0)\n",
    "model_1_0 = als_1_0.fit(train_data)\n",
    "predictions_1_0 = model_1_0.transform(test_data)\n",
    "\n",
    "# Calculate RMSE for Experiment 2\n",
    "squared_error_1_0 = predictions_1_0.withColumn(\"squared_error\", (F.col(\"prediction\") - F.col(\"weight\")) ** 2)\n",
    "rmse_1_0_value = squared_error_1_0.select(F.sqrt(F.avg(\"squared_error\"))).first()[0]\n",
    "print(f\"RMSE with regParam=1.0: {rmse_1_0_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularisation does not seem to have had a significant effect on the RMSE. The RMSE is very high, hence, the model does not seem to give meaningful recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Conclusion\n",
    "We have investigated collaborative filtering through memory-based and model-based methods. First, we implemented simple user-based and item-based methods which were slow, but successfully gave recommendations. The item-based methods seem to run quicker than the user-based methods due to the structure of the data. We then implemented model-based methods - SVD and ALS - which were much more efficient than the previous memory-based methods, and allowed quick recommendations to be given.\n",
    "\n",
    "All of these methods gave recommendations which ranged in quality and performance depending on the availability of data. Next, we used PySpark to implement ALS and this increased the computational efficiency significantly, allowing us to quickly give recommendations for all of the users. This feature would allow us to scale our recommendation methods to large datasets and would be vital in developing our music recommendation system, which could theoretically have millions of users.\n",
    "\n",
    "Finally, as the most efficient method, we evaluated the performance of the PySpark ALS method use RMSE and found this to be very high, leading us to conclude that there is significant room for improvement in our recommendations. However, although our recommender system may not be as good as we would have liked, we have implemented a number of different collaborative filtering methods and seen the benefits of the scalability and efficiency of PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**\n",
    "[1] F.O. Isinkaye, Y.O. Folajimi, B.A. Ojokoh,\n",
    "Recommendation systems: Principles, methods and evaluation,\n",
    "Egyptian Informatics Journal,\n",
    "Volume 16, Issue 3,\n",
    "2015,\n",
    "Pages 261-273.\n",
    "(https://www.sciencedirect.com/science/article/pii/S1110866515000341)\n",
    "\n",
    "[2] Implicit Documentation: https://benfred.github.io/implicit/\n",
    "\n",
    "[3] PySpark Collaborative Filtering Documentation: https://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n",
    "\n",
    "[4] scikit-learn Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "\n",
    "[5] csr_matrix Documentation: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    " \n",
    "[6] PySpark ALS Documentation: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.recommendation.ALS.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
