{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will implement OOP for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation():\n",
    "\n",
    "    user_data = pd.read_csv('../unpushed_work/last_fm_data/user_artists.dat', sep='\\t')\n",
    "\n",
    "    user_interaction_counts = user_data.groupby('userID').size()\n",
    "    users_with_50_interactions = user_interaction_counts[user_interaction_counts >= 50].index\n",
    "    user_data_filtered = user_data[user_data['userID'].isin(users_with_50_interactions)]\n",
    "\n",
    "    unique_users = user_data_filtered['userID'].unique()\n",
    "\n",
    "    # Ensure that test users have at least 50 interactions in 'test_data'\n",
    "    # We need to carefully select 'test_users' to satisfy this condition\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    shuffled_users = np.random.permutation(unique_users)\n",
    "\n",
    "    train_users = []\n",
    "    test_users = []\n",
    "\n",
    "    # We'll collect test users until we have enough that have at least 50 interactions\n",
    "    for user in shuffled_users:\n",
    "        user_data_temp = user_data_filtered[user_data_filtered['userID'] == user]\n",
    "        if len(test_users) < int(0.2 * len(unique_users)):\n",
    "            # Tentatively add to test_users\n",
    "            test_users.append(user)\n",
    "        else:\n",
    "            train_users.append(user)\n",
    "\n",
    "    # Recreate test_data and train_data\n",
    "    train_data = user_data_filtered[user_data_filtered['userID'].isin(train_users)]\n",
    "    test_data = user_data_filtered[user_data_filtered['userID'].isin(test_users)]\n",
    "\n",
    "    # Now check that each user in test_data has 50 interactions\n",
    "    # Remove any users from test_users who don't meet this criterion\n",
    "    valid_test_users = []\n",
    "    for user in test_users:\n",
    "        user_data_temp = test_data[test_data['userID'] == user]\n",
    "        if len(user_data_temp) == 50:\n",
    "            valid_test_users.append(user)\n",
    "\n",
    "    # Update test_users and test_data\n",
    "    test_users = valid_test_users\n",
    "    test_data = test_data[test_data['userID'].isin(test_users)]\n",
    "\n",
    "    # Update train_data to include any users removed from test_users\n",
    "    removed_test_users = set(shuffled_users) - set(train_users) - set(test_users)\n",
    "    if removed_test_users:\n",
    "        train_users.extend(list(removed_test_users))\n",
    "        train_data = user_data_filtered[user_data_filtered['userID'].isin(train_users)]\n",
    "\n",
    "    # Now proceed to split test_data into test_x and test_y\n",
    "    test_x = pd.DataFrame(columns=test_data.columns)\n",
    "    test_y = pd.DataFrame(columns=test_data.columns)\n",
    "\n",
    "    for user in test_users:\n",
    "        user_data_temp = test_data[test_data['userID'] == user]\n",
    "        user_data_shuffled = user_data_temp.sample(frac=1, random_state=42)\n",
    "        user_test_x = user_data_shuffled.iloc[:25]\n",
    "        user_test_y = user_data_shuffled.iloc[25:50]\n",
    "        test_x = pd.concat([test_x, user_test_x], ignore_index=True)\n",
    "        test_y = pd.concat([test_y, user_test_y], ignore_index=True)\n",
    "    \n",
    "    # Encode artistIDs\n",
    "    # Essential since ML models require numerical input (also efficient)\n",
    "    artist_encoder = LabelEncoder()\n",
    "    all_artistIDs = user_data_filtered['artistID'].unique()\n",
    "    artist_encoder.fit(all_artistIDs)\n",
    "    num_artists = len(artist_encoder.classes_)\n",
    "\n",
    "    # Encode train users\n",
    "    train_user_encoder = LabelEncoder()\n",
    "    train_user_encoder.fit(train_users)\n",
    "    num_train_users = len(train_user_encoder.classes_)\n",
    "\n",
    "    # Encode test users\n",
    "    test_user_encoder = LabelEncoder()\n",
    "    test_user_encoder.fit(test_users)\n",
    "    num_test_users = len(test_user_encoder.classes_)\n",
    "\n",
    "    # Function to create user-item interaction matrix\n",
    "    def create_user_item_matrix(data, user_encoder, num_users):\n",
    "        user_item_matrix = np.zeros((num_users, num_artists))\n",
    "        for idx, row in data.iterrows():\n",
    "            user_idx = user_encoder.transform([row['userID']])[0]\n",
    "            artist_idx = artist_encoder.transform([row['artistID']])[0]\n",
    "            weight = row['weight']\n",
    "            user_item_matrix[user_idx, artist_idx] = weight\n",
    "        return user_item_matrix\n",
    "\n",
    "    train_user_item_matrix = create_user_item_matrix(train_data, train_user_encoder, num_train_users)\n",
    "    test_x_user_item_matrix = create_user_item_matrix(test_x, test_user_encoder, num_test_users)\n",
    "    test_y_user_item_matrix = create_user_item_matrix(test_y, test_user_encoder, num_test_users)\n",
    "\n",
    "    # Scale data before inputting it into autoencoder\n",
    "    # Find the maximum and minimum values across all matrices for consistent scaling\n",
    "    max_value = max(train_user_item_matrix.max(), test_x_user_item_matrix.max(), test_y_user_item_matrix.max())\n",
    "    min_value = min(train_user_item_matrix.min(), test_x_user_item_matrix.min(), test_y_user_item_matrix.min())\n",
    "\n",
    "    # Define a function to normalize a matrix\n",
    "    def normalise(matrix, min_value, max_value):\n",
    "        return (matrix - min_value) / (max_value - min_value)\n",
    "\n",
    "    # Normalize each matrix\n",
    "    train_user_item_matrix = normalise(train_user_item_matrix, min_value, max_value)\n",
    "    test_x_user_item_matrix = normalise(test_x_user_item_matrix, min_value, max_value)\n",
    "    test_y_user_item_matrix = normalise(test_y_user_item_matrix, min_value, max_value)\n",
    "\n",
    "    return train_user_item_matrix, test_x_user_item_matrix, test_y_user_item_matrix, artist_encoder, train_user_encoder, test_user_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterise(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std) # Adds noise by sampling from standard normal dist.\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.fc_decode(z))\n",
    "        return torch.sigmoid(self.fc_out(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_user_item_matrix, artist_encoder):\n",
    "    # Convert train data to tensor\n",
    "    train_tensor = torch.FloatTensor(train_user_item_matrix)\n",
    "\n",
    "    # Create DataLoader\n",
    "    batch_size = 64\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize the model, optimizer, and loss function\n",
    "    num_artists = len(artist_encoder.classes_)\n",
    "    input_dim = num_artists\n",
    "    hidden_dim = 256\n",
    "    latent_dim = 50\n",
    "\n",
    "    model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    def loss_function(recon_x, x, mu, logvar):\n",
    "        MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return MSE + KLD\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data_batch in train_loader:\n",
    "            data = data_batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        loss_at_epoch = train_loss / len(train_loader.dataset)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_x_user_item_matrix, test_y_user_item_matrix, metric, k=5):\n",
    "\n",
    "    if metric == 'MSE':\n",
    "        # Convert test data to tensor\n",
    "        test_x_tensor = torch.FloatTensor(test_x_user_item_matrix)\n",
    "        test_y_tensor = torch.FloatTensor(test_y_user_item_matrix)\n",
    "\n",
    "        # Get the model's predictions on test_x\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions, _, _ = model(test_x_tensor) # Reconstructed predictions\n",
    "\n",
    "        mse_loss = nn.functional.mse_loss(predictions, test_y_tensor)\n",
    "        print(f\"Test MSE: {mse_loss.item():.4f}\")\n",
    "        return mse_loss\n",
    "\n",
    "    if metric == 'Hit Rate':\n",
    "        def hit_rate_at_k(true_matrix, predicted_matrix, k):\n",
    "            hits = 0\n",
    "            n_users = true_matrix.shape[0]\n",
    "\n",
    "            for user_idx in range(n_users):\n",
    "\n",
    "                _, top_k_indices = torch.topk(predicted_matrix[user_idx], k=k, largest=True, sorted=True)\n",
    "\n",
    "                if torch.any(true_matrix[user_idx][top_k_indices] > 0):\n",
    "                    hits += 1\n",
    "\n",
    "            return hits / n_users\n",
    "        \n",
    "        hit_rate = hit_rate_at_k(test_y_tensor, predictions, k)\n",
    "        print(f'Hit Rate: {hit_rate}')\n",
    "        return hit_rate\n",
    "    \n",
    "    if metric == 'NDCG':\n",
    "        def ndcg_at_k(true_matrix, predicted_matrix, k):\n",
    "            n_users = true_matrix.shape[0]\n",
    "            ndcg_scores = torch.zeros(n_users)\n",
    "\n",
    "            for user_idx in range(n_users):\n",
    "                _, top_k_indices = torch.topk(predicted_matrix[user_idx], k=k, largest=True, sorted=True)\n",
    "                \n",
    "                true_relevance = true_matrix[user_idx][top_k_indices]\n",
    "\n",
    "                # Compute DCG@k\n",
    "                gains = true_relevance / torch.log2(torch.arange(2, k + 2, dtype=torch.float32))\n",
    "                dcg = torch.sum(gains)\n",
    "\n",
    "                # Compute IDCG@k (ideal DCG)\n",
    "                sorted_relevance, _ = torch.sort(true_matrix[user_idx], descending=True)\n",
    "                ideal_relevance = sorted_relevance[:k]\n",
    "                ideal_gains = ideal_relevance / torch.log2(torch.arange(2, k + 2, dtype=torch.float32))\n",
    "                idcg = torch.sum(ideal_gains)\n",
    "\n",
    "                # Avoid division by zero in case of no relevant items\n",
    "                ndcg_scores[user_idx] = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "            return torch.mean(ndcg_scores)\n",
    "                \n",
    "        print(f'NDCG: {ndcg_at_k(test_y_tensor, predictions, k)}')\n",
    "        return ndcg_at_k(test_y_tensor, predictions, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_item_matrix, test_x_user_item_matrix, test_y_user_item_matrix, artist_encoder, train_user_encoder, test_user_encoder = data_preparation()\n",
    "\n",
    "model = train_model(train_user_item_matrix, artist_encoder)\n",
    "\n",
    "mse = test_model(model, test_x_user_item_matrix, test_y_user_item_matrix, metric='MSE', k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
