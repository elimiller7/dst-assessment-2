{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Section Overview\n",
    "\n",
    "In this section, we will describe how to access the data, complete some Exploratory Data Analysis in order to prepare our data for use, and split our data into Test and Train datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Data Access\n",
    "\n",
    "We used the Last.FM data which records 92,800 artist listening records from 1892 users. Our data can be accessed [here](https://grouplens.org/datasets/hetrec-2011/) by downloading the .zip file under the header Last.FM. These daatsets are also available in the [data folder](https://github.com/elimiller7/dst-assessment-2/tree/main/data) in our GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Test and Train Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/11 17:29:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.csv('last_fm_data/user_artists.dat', header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 2100 users, we are going to separate 20% of users to use as our test data, and the remaining 80% will be our training data. Assuming that the users are ordered randomly, we can select the first 1664 entries of the dataframe to be our training data. This represents the preferences of the first 33 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.limit(1664)\n",
    "df_test = df.orderBy('userID', ascending=False).limit(436)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each user in the test dataset, we will randomly split their artist preferences into a set for testing our model performance, and a seperate validation set for checking how accurate the model's recommendations were. Again, we will choose to retain 20% of artists for our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+\n",
      "|userID|artistID|weight|\n",
      "+------+--------+------+\n",
      "|   999|      65|   433|\n",
      "|   999|     152|   401|\n",
      "|   999|     154|   218|\n",
      "|   999|     188|   128|\n",
      "|   999|     190|   344|\n",
      "|   999|     227|   100|\n",
      "|   999|     233|    72|\n",
      "|   999|     234|   128|\n",
      "|   999|     321|   422|\n",
      "|   999|     344|   129|\n",
      "|   999|     355|    80|\n",
      "|   999|     377|   327|\n",
      "|   999|     418|   167|\n",
      "|   999|     461|   206|\n",
      "|   999|     486|   140|\n",
      "|   999|     498|   125|\n",
      "|   999|     506|   565|\n",
      "|   999|     511|   307|\n",
      "|   999|     533|   131|\n",
      "|   999|     538|   232|\n",
      "+------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = df_test.select('userID').distinct().collect()\n",
    "\n",
    "for user in users:\n",
    "    user_id = user['userID']\n",
    "    \n",
    "    user_data = df_test.filter(df_test['userID'] == user_id)\n",
    "\n",
    "    x, y = user_data.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    test_df = df_test.union(x)\n",
    "    validation_df = df_test.union(y)\n",
    "\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+\n",
      "|userID|artistID|weight|\n",
      "+------+--------+------+\n",
      "|   999|      65|   433|\n",
      "|   999|     152|   401|\n",
      "|   999|     154|   218|\n",
      "|   999|     188|   128|\n",
      "|   999|     190|   344|\n",
      "|   999|     227|   100|\n",
      "|   999|     233|    72|\n",
      "|   999|     234|   128|\n",
      "|   999|     321|   422|\n",
      "|   999|     344|   129|\n",
      "|   999|     355|    80|\n",
      "|   999|     377|   327|\n",
      "|   999|     418|   167|\n",
      "|   999|     461|   206|\n",
      "|   999|     486|   140|\n",
      "|   999|     498|   125|\n",
      "|   999|     506|   565|\n",
      "|   999|     511|   307|\n",
      "|   999|     533|   131|\n",
      "|   999|     538|   232|\n",
      "+------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
